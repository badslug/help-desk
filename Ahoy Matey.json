{
	"data": {
		"edges": [
			{
				"animated": false,
				"className": "",
				"data": {
					"sourceHandle": {
						"dataType": "ChatInput",
						"id": "ChatInput-yIvxw",
						"name": "message",
						"output_types": ["Message"]
					},
					"targetHandle": {
						"fieldName": "message",
						"id": "Prompt-FSzc4",
						"inputTypes": ["Message"],
						"type": "str"
					}
				},
				"id": "xy-edge__ChatInput-yIvxw{œdataTypeœ:œChatInputœ,œidœ:œChatInput-yIvxwœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-FSzc4{œfieldNameœ:œmessageœ,œidœ:œPrompt-FSzc4œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
				"selected": false,
				"source": "ChatInput-yIvxw",
				"sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-yIvxwœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
				"target": "Prompt-FSzc4",
				"targetHandle": "{œfieldNameœ:œmessageœ,œidœ:œPrompt-FSzc4œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
			},
			{
				"animated": false,
				"className": "",
				"data": {
					"sourceHandle": {
						"dataType": "Prompt",
						"id": "Prompt-FSzc4",
						"name": "prompt",
						"output_types": ["Message"]
					},
					"targetHandle": {
						"fieldName": "input_value",
						"id": "OllamaModel-ErZ5p",
						"inputTypes": ["Message"],
						"type": "str"
					}
				},
				"id": "xy-edge__Prompt-FSzc4{œdataTypeœ:œPromptœ,œidœ:œPrompt-FSzc4œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OllamaModel-ErZ5p{œfieldNameœ:œinput_valueœ,œidœ:œOllamaModel-ErZ5pœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
				"selected": false,
				"source": "Prompt-FSzc4",
				"sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-FSzc4œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
				"target": "OllamaModel-ErZ5p",
				"targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œOllamaModel-ErZ5pœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
			},
			{
				"animated": false,
				"className": "",
				"data": {
					"sourceHandle": {
						"dataType": "SplitText",
						"id": "SplitText-s8WZO",
						"name": "chunks",
						"output_types": ["Data"]
					},
					"targetHandle": {
						"fieldName": "ingest_data",
						"id": "Chroma-3QY7S",
						"inputTypes": ["Data", "DataFrame"],
						"type": "other"
					}
				},
				"id": "xy-edge__SplitText-s8WZO{œdataTypeœ:œSplitTextœ,œidœ:œSplitText-s8WZOœ,œnameœ:œchunksœ,œoutput_typesœ:[œDataœ]}-Chroma-3QY7S{œfieldNameœ:œingest_dataœ,œidœ:œChroma-3QY7Sœ,œinputTypesœ:[œDataœ,œDataFrameœ],œtypeœ:œotherœ}",
				"selected": false,
				"source": "SplitText-s8WZO",
				"sourceHandle": "{œdataTypeœ:œSplitTextœ,œidœ:œSplitText-s8WZOœ,œnameœ:œchunksœ,œoutput_typesœ:[œDataœ]}",
				"target": "Chroma-3QY7S",
				"targetHandle": "{œfieldNameœ:œingest_dataœ,œidœ:œChroma-3QY7Sœ,œinputTypesœ:[œDataœ,œDataFrameœ],œtypeœ:œotherœ}"
			},
			{
				"animated": false,
				"className": "",
				"data": {
					"sourceHandle": {
						"dataType": "OllamaEmbeddings",
						"id": "OllamaEmbeddings-DxU0f",
						"name": "embeddings",
						"output_types": ["Embeddings"]
					},
					"targetHandle": {
						"fieldName": "embedding",
						"id": "Chroma-3QY7S",
						"inputTypes": ["Embeddings"],
						"type": "other"
					}
				},
				"id": "xy-edge__OllamaEmbeddings-DxU0f{œdataTypeœ:œOllamaEmbeddingsœ,œidœ:œOllamaEmbeddings-DxU0fœ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}-Chroma-3QY7S{œfieldNameœ:œembeddingœ,œidœ:œChroma-3QY7Sœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}",
				"selected": false,
				"source": "OllamaEmbeddings-DxU0f",
				"sourceHandle": "{œdataTypeœ:œOllamaEmbeddingsœ,œidœ:œOllamaEmbeddings-DxU0fœ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}",
				"target": "Chroma-3QY7S",
				"targetHandle": "{œfieldNameœ:œembeddingœ,œidœ:œChroma-3QY7Sœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}"
			},
			{
				"animated": false,
				"className": "",
				"data": {
					"sourceHandle": {
						"dataType": "Chroma",
						"id": "Chroma-3QY7S",
						"name": "search_results",
						"output_types": ["Data"]
					},
					"targetHandle": {
						"fieldName": "input_data",
						"id": "ParserComponent-NWgqo",
						"inputTypes": ["DataFrame", "Data"],
						"type": "other"
					}
				},
				"id": "xy-edge__Chroma-3QY7S{œdataTypeœ:œChromaœ,œidœ:œChroma-3QY7Sœ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}-ParserComponent-NWgqo{œfieldNameœ:œinput_dataœ,œidœ:œParserComponent-NWgqoœ,œinputTypesœ:[œDataFrameœ,œDataœ],œtypeœ:œotherœ}",
				"selected": false,
				"source": "Chroma-3QY7S",
				"sourceHandle": "{œdataTypeœ:œChromaœ,œidœ:œChroma-3QY7Sœ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}",
				"target": "ParserComponent-NWgqo",
				"targetHandle": "{œfieldNameœ:œinput_dataœ,œidœ:œParserComponent-NWgqoœ,œinputTypesœ:[œDataFrameœ,œDataœ],œtypeœ:œotherœ}"
			},
			{
				"animated": false,
				"className": "",
				"data": {
					"sourceHandle": {
						"dataType": "Memory",
						"id": "Memory-tvjD7",
						"name": "messages_text",
						"output_types": ["Message"]
					},
					"targetHandle": {
						"fieldName": "history",
						"id": "Prompt-FSzc4",
						"inputTypes": ["Message"],
						"type": "str"
					}
				},
				"id": "xy-edge__Memory-tvjD7{œdataTypeœ:œMemoryœ,œidœ:œMemory-tvjD7œ,œnameœ:œmessages_textœ,œoutput_typesœ:[œMessageœ]}-Prompt-FSzc4{œfieldNameœ:œhistoryœ,œidœ:œPrompt-FSzc4œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
				"selected": false,
				"source": "Memory-tvjD7",
				"sourceHandle": "{œdataTypeœ:œMemoryœ,œidœ:œMemory-tvjD7œ,œnameœ:œmessages_textœ,œoutput_typesœ:[œMessageœ]}",
				"target": "Prompt-FSzc4",
				"targetHandle": "{œfieldNameœ:œhistoryœ,œidœ:œPrompt-FSzc4œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
			},
			{
				"animated": false,
				"className": "",
				"data": {
					"sourceHandle": {
						"dataType": "OllamaModel",
						"id": "OllamaModel-ErZ5p",
						"name": "text_output",
						"output_types": ["Message"]
					},
					"targetHandle": {
						"fieldName": "input_value",
						"id": "ChatOutput-2vEXM",
						"inputTypes": ["Data", "DataFrame", "Message"],
						"type": "other"
					}
				},
				"id": "xy-edge__OllamaModel-ErZ5p{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-ErZ5pœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-2vEXM{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-2vEXMœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}",
				"selected": false,
				"source": "OllamaModel-ErZ5p",
				"sourceHandle": "{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-ErZ5pœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
				"target": "ChatOutput-2vEXM",
				"targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-2vEXMœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}"
			},
			{
				"animated": false,
				"className": "",
				"data": {
					"sourceHandle": {
						"dataType": "ParserComponent",
						"id": "ParserComponent-NWgqo",
						"name": "parsed_text",
						"output_types": ["Message"]
					},
					"targetHandle": {
						"fieldName": "chunks",
						"id": "Prompt-FSzc4",
						"inputTypes": ["Message"],
						"type": "str"
					}
				},
				"id": "xy-edge__ParserComponent-NWgqo{œdataTypeœ:œParserComponentœ,œidœ:œParserComponent-NWgqoœ,œnameœ:œparsed_textœ,œoutput_typesœ:[œMessageœ]}-Prompt-FSzc4{œfieldNameœ:œchunksœ,œidœ:œPrompt-FSzc4œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
				"selected": false,
				"source": "ParserComponent-NWgqo",
				"sourceHandle": "{œdataTypeœ:œParserComponentœ,œidœ:œParserComponent-NWgqoœ,œnameœ:œparsed_textœ,œoutput_typesœ:[œMessageœ]}",
				"target": "Prompt-FSzc4",
				"targetHandle": "{œfieldNameœ:œchunksœ,œidœ:œPrompt-FSzc4œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
			},
			{
				"animated": false,
				"className": "",
				"data": {
					"sourceHandle": {
						"dataType": "ChatInput",
						"id": "ChatInput-yIvxw",
						"name": "message",
						"output_types": ["Message"]
					},
					"targetHandle": {
						"fieldName": "search_query",
						"id": "Chroma-3QY7S",
						"inputTypes": ["Message"],
						"type": "query"
					}
				},
				"id": "xy-edge__ChatInput-yIvxw{œdataTypeœ:œChatInputœ,œidœ:œChatInput-yIvxwœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Chroma-3QY7S{œfieldNameœ:œsearch_queryœ,œidœ:œChroma-3QY7Sœ,œinputTypesœ:[œMessageœ],œtypeœ:œqueryœ}",
				"selected": false,
				"source": "ChatInput-yIvxw",
				"sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-yIvxwœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
				"target": "Chroma-3QY7S",
				"targetHandle": "{œfieldNameœ:œsearch_queryœ,œidœ:œChroma-3QY7Sœ,œinputTypesœ:[œMessageœ],œtypeœ:œqueryœ}"
			},
			{
				"animated": false,
				"className": "",
				"data": {
					"sourceHandle": {
						"dataType": "URLComponent",
						"id": "URLComponent-wTaji",
						"name": "data",
						"output_types": ["Data"]
					},
					"targetHandle": {
						"fieldName": "data_inputs",
						"id": "SplitText-s8WZO",
						"inputTypes": ["Data", "DataFrame"],
						"type": "other"
					}
				},
				"id": "xy-edge__URLComponent-wTaji{œdataTypeœ:œURLComponentœ,œidœ:œURLComponent-wTajiœ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}-SplitText-s8WZO{œfieldNameœ:œdata_inputsœ,œidœ:œSplitText-s8WZOœ,œinputTypesœ:[œDataœ,œDataFrameœ],œtypeœ:œotherœ}",
				"selected": false,
				"source": "URLComponent-wTaji",
				"sourceHandle": "{œdataTypeœ:œURLComponentœ,œidœ:œURLComponent-wTajiœ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}",
				"target": "SplitText-s8WZO",
				"targetHandle": "{œfieldNameœ:œdata_inputsœ,œidœ:œSplitText-s8WZOœ,œinputTypesœ:[œDataœ,œDataFrameœ],œtypeœ:œotherœ}"
			}
		],
		"nodes": [
			{
				"data": {
					"id": "ChatInput-yIvxw",
					"node": {
						"base_classes": ["Message"],
						"beta": false,
						"conditional_paths": [],
						"custom_fields": {},
						"description": "Get chat inputs from the Playground.",
						"display_name": "Chat Input",
						"documentation": "",
						"edited": false,
						"field_order": [
							"input_value",
							"should_store_message",
							"sender",
							"sender_name",
							"session_id",
							"files",
							"background_color",
							"chat_icon",
							"text_color"
						],
						"frozen": false,
						"icon": "MessagesSquare",
						"legacy": false,
						"lf_version": "1.4.2.dev8",
						"metadata": {},
						"minimized": true,
						"output_types": [],
						"outputs": [
							{
								"allows_loop": false,
								"cache": true,
								"display_name": "Message",
								"method": "message_response",
								"name": "message",
								"selected": "Message",
								"tool_mode": true,
								"types": ["Message"],
								"value": "__UNDEFINED__"
							}
						],
						"pinned": false,
						"template": {
							"_type": "Component",
							"background_color": {
								"_input_type": "MessageTextInput",
								"advanced": true,
								"display_name": "Background Color",
								"dynamic": false,
								"info": "The background color of the icon.",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "background_color",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": ""
							},
							"chat_icon": {
								"_input_type": "MessageTextInput",
								"advanced": true,
								"display_name": "Icon",
								"dynamic": false,
								"info": "The icon of the message.",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "chat_icon",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": ""
							},
							"code": {
								"advanced": true,
								"dynamic": true,
								"fileTypes": [],
								"file_path": "",
								"info": "",
								"list": false,
								"load_from_db": false,
								"multiline": true,
								"name": "code",
								"password": false,
								"placeholder": "",
								"required": true,
								"show": true,
								"title_case": false,
								"type": "code",
								"value": "from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import (\n    DropdownInput,\n    FileInput,\n    MessageTextInput,\n    MultilineInput,\n    Output,\n)\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_USER,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"MessagesSquare\"\n    name = \"ChatInput\"\n    minimized = True\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n            input_types=[],\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n            temp_file=True,\n        ),\n        MessageTextInput(\n            name=\"background_color\",\n            display_name=\"Background Color\",\n            info=\"The background color of the icon.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"text_color\",\n            display_name=\"Text Color\",\n            info=\"The text color of the name\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    async def message_response(self) -> Message:\n        background_color = self.background_color\n        text_color = self.text_color\n        icon = self.chat_icon\n\n        message = await Message.create(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n            properties={\n                \"background_color\": background_color,\n                \"text_color\": text_color,\n                \"icon\": icon,\n            },\n        )\n        if self.session_id and isinstance(message, Message) and self.should_store_message:\n            stored_message = await self.send_message(\n                message,\n            )\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n"
							},
							"files": {
								"_input_type": "FileInput",
								"advanced": true,
								"display_name": "Files",
								"dynamic": false,
								"fileTypes": [
									"txt",
									"md",
									"mdx",
									"csv",
									"json",
									"yaml",
									"yml",
									"xml",
									"html",
									"htm",
									"pdf",
									"docx",
									"py",
									"sh",
									"sql",
									"js",
									"ts",
									"tsx",
									"jpg",
									"jpeg",
									"png",
									"bmp",
									"image"
								],
								"file_path": "",
								"info": "Files to be sent with the message.",
								"list": true,
								"list_add_label": "Add More",
								"name": "files",
								"placeholder": "",
								"required": false,
								"show": true,
								"temp_file": true,
								"title_case": false,
								"trace_as_metadata": true,
								"type": "file",
								"value": ""
							},
							"input_value": {
								"_input_type": "MultilineInput",
								"advanced": false,
								"copy_field": false,
								"display_name": "Text",
								"dynamic": false,
								"info": "Message to be passed as input.",
								"input_types": [],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"multiline": true,
								"name": "input_value",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": ""
							},
							"sender": {
								"_input_type": "DropdownInput",
								"advanced": true,
								"combobox": false,
								"dialog_inputs": {},
								"display_name": "Sender Type",
								"dynamic": false,
								"info": "Type of sender.",
								"name": "sender",
								"options": ["Machine", "User"],
								"options_metadata": [],
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"toggle": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "str",
								"value": "User"
							},
							"sender_name": {
								"_input_type": "MessageTextInput",
								"advanced": true,
								"display_name": "Sender Name",
								"dynamic": false,
								"info": "Name of the sender.",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "sender_name",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": "User"
							},
							"session_id": {
								"_input_type": "MessageTextInput",
								"advanced": true,
								"display_name": "Session ID",
								"dynamic": false,
								"info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "session_id",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": ""
							},
							"should_store_message": {
								"_input_type": "BoolInput",
								"advanced": true,
								"display_name": "Store Messages",
								"dynamic": false,
								"info": "Store the message in the history.",
								"list": false,
								"list_add_label": "Add More",
								"name": "should_store_message",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "bool",
								"value": true
							},
							"text_color": {
								"_input_type": "MessageTextInput",
								"advanced": true,
								"display_name": "Text Color",
								"dynamic": false,
								"info": "The text color of the name",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "text_color",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": ""
							}
						},
						"tool_mode": false
					},
					"showNode": false,
					"type": "ChatInput"
				},
				"dragging": false,
				"id": "ChatInput-yIvxw",
				"measured": {
					"height": 48,
					"width": 192
				},
				"position": {
					"x": -270.35072609537895,
					"y": 1767.2674246964868
				},
				"selected": false,
				"type": "genericNode"
			},
			{
				"data": {
					"id": "OllamaModel-ErZ5p",
					"node": {
						"base_classes": ["LanguageModel", "Message"],
						"beta": false,
						"conditional_paths": [],
						"custom_fields": {},
						"description": "Generate text using Ollama Local LLMs.",
						"display_name": "Ollama",
						"documentation": "",
						"edited": false,
						"field_order": [
							"base_url",
							"model_name",
							"temperature",
							"format",
							"metadata",
							"mirostat",
							"mirostat_eta",
							"mirostat_tau",
							"num_ctx",
							"num_gpu",
							"num_thread",
							"repeat_last_n",
							"repeat_penalty",
							"tfs_z",
							"timeout",
							"top_k",
							"top_p",
							"verbose",
							"tags",
							"stop_tokens",
							"system",
							"tool_model_enabled",
							"template",
							"input_value",
							"system_message",
							"stream"
						],
						"frozen": false,
						"icon": "Ollama",
						"legacy": false,
						"lf_version": "1.4.2.dev8",
						"metadata": {
							"keywords": ["model", "llm", "language model", "large language model"]
						},
						"minimized": false,
						"output_types": [],
						"outputs": [
							{
								"allows_loop": false,
								"cache": true,
								"display_name": "Message",
								"hidden": false,
								"method": "text_response",
								"name": "text_output",
								"options": null,
								"required_inputs": [],
								"selected": "Message",
								"tool_mode": true,
								"types": ["Message"],
								"value": "__UNDEFINED__"
							},
							{
								"allows_loop": false,
								"cache": true,
								"display_name": "Language Model",
								"method": "build_model",
								"name": "model_output",
								"options": null,
								"required_inputs": [],
								"selected": "LanguageModel",
								"tool_mode": true,
								"types": ["LanguageModel"],
								"value": "__UNDEFINED__"
							}
						],
						"pinned": false,
						"template": {
							"_type": "Component",
							"base_url": {
								"_input_type": "MessageTextInput",
								"advanced": false,
								"display_name": "Base URL",
								"dynamic": false,
								"info": "Endpoint of the Ollama API.",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "base_url",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": "http://localhost:11434/"
							},
							"code": {
								"advanced": true,
								"dynamic": true,
								"fileTypes": [],
								"file_path": "",
								"info": "",
								"list": false,
								"load_from_db": false,
								"multiline": true,
								"name": "code",
								"password": false,
								"placeholder": "",
								"required": true,
								"show": true,
								"title_case": false,
								"type": "code",
								"value": "import asyncio\nfrom typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_ollama import ChatOllama\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.ollama_constants import URL_LIST\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, MessageTextInput, SliderInput\nfrom langflow.logging import logger\n\nHTTP_STATUS_OK = 200\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    # Define constants for JSON keys\n    JSON_MODELS_KEY = \"models\"\n    JSON_NAME_KEY = \"name\"\n    JSON_CAPABILITIES_KEY = \"capabilities\"\n    DESIRED_CAPABILITY = \"completion\"\n    TOOL_CALLING_CAPABILITY = \"tools\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API.\",\n            value=\"\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            options=[],\n            info=\"Refer to https://ollama.com/library for more models.\",\n            refresh_button=True,\n            real_time_refresh=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=1, step=0.01),\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"format\", display_name=\"Format\", info=\"Specify the format of the output (e.g., json).\", advanced=True\n        ),\n        DictInput(name=\"metadata\", display_name=\"Metadata\", info=\"Metadata to add to the run trace.\", advanced=True),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(name=\"tfs_z\", display_name=\"TFS Z\", info=\"Tail free sampling value. (Default: 1)\", advanced=True),\n        IntInput(name=\"timeout\", display_name=\"Timeout\", info=\"Timeout for the request stream.\", advanced=True),\n        IntInput(\n            name=\"top_k\", display_name=\"Top K\", info=\"Limits token selection to top K. (Default: 40)\", advanced=True\n        ),\n        FloatInput(name=\"top_p\", display_name=\"Top P\", info=\"Works together with top-k. (Default: 0.9)\", advanced=True),\n        BoolInput(name=\"verbose\", display_name=\"Verbose\", info=\"Whether to print out response text.\", advanced=True),\n        MessageTextInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"system\", display_name=\"System\", info=\"System to use for generating text.\", advanced=True\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to enable tool calling in the model.\",\n            value=True,\n            real_time_refresh=True,\n        ),\n        MessageTextInput(\n            name=\"template\", display_name=\"Template\", info=\"Template to use for generating text.\", advanced=True\n        ),\n        *LCModelComponent._base_inputs,\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model_name,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n            \"template\": self.template,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)\n        except Exception as e:\n            msg = (\n                \"Unable to connect to the Ollama API. \",\n                \"Please verify the base URL, ensure the relevant Ollama model is pulled, and try again.\",\n            )\n            raise ValueError(msg) from e\n\n        return output\n\n    async def is_valid_ollama_url(self, url: str) -> bool:\n        try:\n            async with httpx.AsyncClient() as client:\n                return (await client.get(urljoin(url, \"api/tags\"))).status_code == HTTP_STATUS_OK\n        except httpx.RequestError:\n            return False\n\n    async def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name in {\"base_url\", \"model_name\"}:\n            if build_config[\"base_url\"].get(\"load_from_db\", False):\n                base_url_value = await self.get_variables(build_config[\"base_url\"].get(\"value\", \"\"), \"base_url\")\n            else:\n                base_url_value = build_config[\"base_url\"].get(\"value\", \"\")\n\n            if not await self.is_valid_ollama_url(base_url_value):\n                # Check if any URL in the list is valid\n                valid_url = \"\"\n                check_urls = URL_LIST\n                if self.base_url:\n                    check_urls = [self.base_url, *URL_LIST]\n                for url in check_urls:\n                    if await self.is_valid_ollama_url(url):\n                        valid_url = url\n                        break\n                if valid_url != \"\":\n                    build_config[\"base_url\"][\"value\"] = valid_url\n                else:\n                    msg = \"No valid Ollama URL found.\"\n                    raise ValueError(msg)\n        if field_name in {\"model_name\", \"base_url\", \"tool_model_enabled\"}:\n            if await self.is_valid_ollama_url(self.base_url):\n                tool_model_enabled = build_config[\"tool_model_enabled\"].get(\"value\", False) or self.tool_model_enabled\n                build_config[\"model_name\"][\"options\"] = await self.get_models(self.base_url, tool_model_enabled)\n            elif await self.is_valid_ollama_url(build_config[\"base_url\"].get(\"value\", \"\")):\n                tool_model_enabled = build_config[\"tool_model_enabled\"].get(\"value\", False) or self.tool_model_enabled\n                build_config[\"model_name\"][\"options\"] = await self.get_models(\n                    build_config[\"base_url\"].get(\"value\", \"\"), tool_model_enabled\n                )\n            else:\n                build_config[\"model_name\"][\"options\"] = []\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    async def get_models(self, base_url_value: str, tool_model_enabled: bool | None = None) -> list[str]:\n        \"\"\"Fetches a list of models from the Ollama API that do not have the \"embedding\" capability.\n\n        Args:\n            base_url_value (str): The base URL of the Ollama API.\n            tool_model_enabled (bool | None, optional): If True, filters the models further to include\n                only those that support tool calling. Defaults to None.\n\n        Returns:\n            list[str]: A list of model names that do not have the \"embedding\" capability. If\n                `tool_model_enabled` is True, only models supporting tool calling are included.\n\n        Raises:\n            ValueError: If there is an issue with the API request or response, or if the model\n                names cannot be retrieved.\n        \"\"\"\n        try:\n            # Normalize the base URL to avoid the repeated \"/\" at the end\n            base_url = base_url_value.rstrip(\"/\") + \"/\"\n\n            # Ollama REST API to return models\n            tags_url = urljoin(base_url, \"api/tags\")\n\n            # Ollama REST API to return model capabilities\n            show_url = urljoin(base_url, \"api/show\")\n\n            async with httpx.AsyncClient() as client:\n                # Fetch available models\n                tags_response = await client.get(tags_url)\n                tags_response.raise_for_status()\n                models = tags_response.json()\n                if asyncio.iscoroutine(models):\n                    models = await models\n                logger.debug(f\"Available models: {models}\")\n\n                # Filter models that are NOT embedding models\n                model_ids = []\n                for model in models[self.JSON_MODELS_KEY]:\n                    model_name = model[self.JSON_NAME_KEY]\n                    logger.debug(f\"Checking model: {model_name}\")\n\n                    payload = {\"model\": model_name}\n                    show_response = await client.post(show_url, json=payload)\n                    show_response.raise_for_status()\n                    json_data = show_response.json()\n                    if asyncio.iscoroutine(json_data):\n                        json_data = await json_data\n                    capabilities = json_data.get(self.JSON_CAPABILITIES_KEY, [])\n                    logger.debug(f\"Model: {model_name}, Capabilities: {capabilities}\")\n\n                    if self.DESIRED_CAPABILITY in capabilities and (\n                        not tool_model_enabled or self.TOOL_CALLING_CAPABILITY in capabilities\n                    ):\n                        model_ids.append(model_name)\n\n        except (httpx.RequestError, ValueError) as e:\n            msg = \"Could not get model names from Ollama.\"\n            raise ValueError(msg) from e\n\n        return model_ids\n"
							},
							"format": {
								"_input_type": "MessageTextInput",
								"advanced": true,
								"display_name": "Format",
								"dynamic": false,
								"info": "Specify the format of the output (e.g., json).",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "format",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": ""
							},
							"input_value": {
								"_input_type": "MessageInput",
								"advanced": false,
								"display_name": "Input",
								"dynamic": false,
								"info": "",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "input_value",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": ""
							},
							"metadata": {
								"_input_type": "DictInput",
								"advanced": true,
								"display_name": "Metadata",
								"dynamic": false,
								"info": "Metadata to add to the run trace.",
								"list": false,
								"list_add_label": "Add More",
								"name": "metadata",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"type": "dict",
								"value": {}
							},
							"mirostat": {
								"_input_type": "DropdownInput",
								"advanced": true,
								"combobox": false,
								"dialog_inputs": {},
								"display_name": "Mirostat",
								"dynamic": false,
								"info": "Enable/disable Mirostat sampling for controlling perplexity.",
								"name": "mirostat",
								"options": ["Disabled", "Mirostat", "Mirostat 2.0"],
								"options_metadata": [],
								"placeholder": "",
								"real_time_refresh": true,
								"required": false,
								"show": true,
								"title_case": false,
								"toggle": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "str",
								"value": "Disabled"
							},
							"mirostat_eta": {
								"_input_type": "FloatInput",
								"advanced": true,
								"display_name": "Mirostat Eta",
								"dynamic": false,
								"info": "Learning rate for Mirostat algorithm. (Default: 0.1)",
								"list": false,
								"list_add_label": "Add More",
								"name": "mirostat_eta",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "float",
								"value": ""
							},
							"mirostat_tau": {
								"_input_type": "FloatInput",
								"advanced": true,
								"display_name": "Mirostat Tau",
								"dynamic": false,
								"info": "Controls the balance between coherence and diversity of the output. (Default: 5.0)",
								"list": false,
								"list_add_label": "Add More",
								"name": "mirostat_tau",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "float",
								"value": ""
							},
							"model_name": {
								"_input_type": "DropdownInput",
								"advanced": false,
								"combobox": false,
								"dialog_inputs": {},
								"display_name": "Model Name",
								"dynamic": false,
								"info": "Refer to https://ollama.com/library for more models.",
								"name": "model_name",
								"options": ["qwen2.5:latest"],
								"options_metadata": [],
								"placeholder": "",
								"real_time_refresh": true,
								"refresh_button": true,
								"required": false,
								"show": true,
								"title_case": false,
								"toggle": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "str",
								"value": "qwen2.5:latest"
							},
							"num_ctx": {
								"_input_type": "IntInput",
								"advanced": true,
								"display_name": "Context Window Size",
								"dynamic": false,
								"info": "Size of the context window for generating tokens. (Default: 2048)",
								"list": false,
								"list_add_label": "Add More",
								"name": "num_ctx",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "int",
								"value": ""
							},
							"num_gpu": {
								"_input_type": "IntInput",
								"advanced": true,
								"display_name": "Number of GPUs",
								"dynamic": false,
								"info": "Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)",
								"list": false,
								"list_add_label": "Add More",
								"name": "num_gpu",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "int",
								"value": ""
							},
							"num_thread": {
								"_input_type": "IntInput",
								"advanced": true,
								"display_name": "Number of Threads",
								"dynamic": false,
								"info": "Number of threads to use during computation. (Default: detected for optimal performance)",
								"list": false,
								"list_add_label": "Add More",
								"name": "num_thread",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "int",
								"value": ""
							},
							"repeat_last_n": {
								"_input_type": "IntInput",
								"advanced": true,
								"display_name": "Repeat Last N",
								"dynamic": false,
								"info": "How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)",
								"list": false,
								"list_add_label": "Add More",
								"name": "repeat_last_n",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "int",
								"value": ""
							},
							"repeat_penalty": {
								"_input_type": "FloatInput",
								"advanced": true,
								"display_name": "Repeat Penalty",
								"dynamic": false,
								"info": "Penalty for repetitions in generated text. (Default: 1.1)",
								"list": false,
								"list_add_label": "Add More",
								"name": "repeat_penalty",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "float",
								"value": ""
							},
							"stop_tokens": {
								"_input_type": "MessageTextInput",
								"advanced": true,
								"display_name": "Stop Tokens",
								"dynamic": false,
								"info": "Comma-separated list of tokens to signal the model to stop generating text.",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "stop_tokens",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": ""
							},
							"stream": {
								"_input_type": "BoolInput",
								"advanced": true,
								"display_name": "Stream",
								"dynamic": false,
								"info": "Stream the response from the model. Streaming works only in Chat.",
								"list": false,
								"list_add_label": "Add More",
								"name": "stream",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "bool",
								"value": false
							},
							"system": {
								"_input_type": "MessageTextInput",
								"advanced": true,
								"display_name": "System",
								"dynamic": false,
								"info": "System to use for generating text.",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "system",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": ""
							},
							"system_message": {
								"_input_type": "MultilineInput",
								"advanced": false,
								"copy_field": false,
								"display_name": "System Message",
								"dynamic": false,
								"info": "System message to pass to the model.",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"multiline": true,
								"name": "system_message",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": ""
							},
							"tags": {
								"_input_type": "MessageTextInput",
								"advanced": true,
								"display_name": "Tags",
								"dynamic": false,
								"info": "Comma-separated list of tags to add to the run trace.",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "tags",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": ""
							},
							"temperature": {
								"_input_type": "SliderInput",
								"advanced": true,
								"display_name": "Temperature",
								"dynamic": false,
								"info": "",
								"max_label": "",
								"max_label_icon": "",
								"min_label": "",
								"min_label_icon": "",
								"name": "temperature",
								"placeholder": "",
								"range_spec": {
									"max": 1,
									"min": 0,
									"step": 0.01,
									"step_type": "float"
								},
								"required": false,
								"show": true,
								"slider_buttons": false,
								"slider_buttons_options": [],
								"slider_input": false,
								"title_case": false,
								"tool_mode": false,
								"type": "slider",
								"value": 0.1
							},
							"template": {
								"_input_type": "MessageTextInput",
								"advanced": true,
								"display_name": "Template",
								"dynamic": false,
								"info": "Template to use for generating text.",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "template",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": ""
							},
							"tfs_z": {
								"_input_type": "FloatInput",
								"advanced": true,
								"display_name": "TFS Z",
								"dynamic": false,
								"info": "Tail free sampling value. (Default: 1)",
								"list": false,
								"list_add_label": "Add More",
								"name": "tfs_z",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "float",
								"value": ""
							},
							"timeout": {
								"_input_type": "IntInput",
								"advanced": true,
								"display_name": "Timeout",
								"dynamic": false,
								"info": "Timeout for the request stream.",
								"list": false,
								"list_add_label": "Add More",
								"name": "timeout",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "int",
								"value": ""
							},
							"tool_model_enabled": {
								"_input_type": "BoolInput",
								"advanced": false,
								"display_name": "Tool Model Enabled",
								"dynamic": false,
								"info": "Whether to enable tool calling in the model.",
								"list": false,
								"list_add_label": "Add More",
								"name": "tool_model_enabled",
								"placeholder": "",
								"real_time_refresh": true,
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "bool",
								"value": true
							},
							"top_k": {
								"_input_type": "IntInput",
								"advanced": true,
								"display_name": "Top K",
								"dynamic": false,
								"info": "Limits token selection to top K. (Default: 40)",
								"list": false,
								"list_add_label": "Add More",
								"name": "top_k",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "int",
								"value": ""
							},
							"top_p": {
								"_input_type": "FloatInput",
								"advanced": true,
								"display_name": "Top P",
								"dynamic": false,
								"info": "Works together with top-k. (Default: 0.9)",
								"list": false,
								"list_add_label": "Add More",
								"name": "top_p",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "float",
								"value": ""
							},
							"verbose": {
								"_input_type": "BoolInput",
								"advanced": true,
								"display_name": "Verbose",
								"dynamic": false,
								"info": "Whether to print out response text.",
								"list": false,
								"list_add_label": "Add More",
								"name": "verbose",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "bool",
								"value": false
							}
						},
						"tool_mode": false
					},
					"showNode": true,
					"type": "OllamaModel"
				},
				"dragging": false,
				"id": "OllamaModel-ErZ5p",
				"measured": {
					"height": 497,
					"width": 320
				},
				"position": {
					"x": 1981.7336973241945,
					"y": 1327.2795710926366
				},
				"selected": false,
				"type": "genericNode"
			},
			{
				"data": {
					"id": "ChatOutput-2vEXM",
					"node": {
						"base_classes": ["Message"],
						"beta": false,
						"conditional_paths": [],
						"custom_fields": {},
						"description": "Display a chat message in the Playground.",
						"display_name": "Chat Output",
						"documentation": "",
						"edited": false,
						"field_order": [
							"input_value",
							"should_store_message",
							"sender",
							"sender_name",
							"session_id",
							"data_template",
							"background_color",
							"chat_icon",
							"text_color",
							"clean_data"
						],
						"frozen": false,
						"icon": "MessagesSquare",
						"legacy": false,
						"lf_version": "1.4.2.dev8",
						"metadata": {},
						"minimized": true,
						"output_types": [],
						"outputs": [
							{
								"allows_loop": false,
								"cache": true,
								"display_name": "Message",
								"method": "message_response",
								"name": "message",
								"selected": "Message",
								"tool_mode": true,
								"types": ["Message"],
								"value": "__UNDEFINED__"
							}
						],
						"pinned": false,
						"template": {
							"_type": "Component",
							"background_color": {
								"_input_type": "MessageTextInput",
								"advanced": true,
								"display_name": "Background Color",
								"dynamic": false,
								"info": "The background color of the icon.",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "background_color",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": ""
							},
							"chat_icon": {
								"_input_type": "MessageTextInput",
								"advanced": true,
								"display_name": "Icon",
								"dynamic": false,
								"info": "The icon of the message.",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "chat_icon",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": ""
							},
							"clean_data": {
								"_input_type": "BoolInput",
								"advanced": true,
								"display_name": "Basic Clean Data",
								"dynamic": false,
								"info": "Whether to clean the data",
								"list": false,
								"list_add_label": "Add More",
								"name": "clean_data",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "bool",
								"value": true
							},
							"code": {
								"advanced": true,
								"dynamic": true,
								"fileTypes": [],
								"file_path": "",
								"info": "",
								"list": false,
								"load_from_db": false,
								"multiline": true,
								"name": "code",
								"password": false,
								"placeholder": "",
								"required": true,
								"show": true,
								"title_case": false,
								"type": "code",
								"value": "from collections.abc import Generator\nfrom typing import Any\n\nimport orjson\nfrom fastapi.encoders import jsonable_encoder\n\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.inputs.inputs import HandleInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.schema.data import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.schema.properties import Source\nfrom langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_AI,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n    minimized = True\n\n    inputs = [\n        HandleInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n            input_types=[\"Data\", \"DataFrame\", \"Message\"],\n            required=True,\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n        MessageTextInput(\n            name=\"background_color\",\n            display_name=\"Background Color\",\n            info=\"The background color of the icon.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"text_color\",\n            display_name=\"Text Color\",\n            info=\"The text color of the name\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"clean_data\",\n            display_name=\"Basic Clean Data\",\n            value=True,\n            info=\"Whether to clean the data\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n        Output(\n            display_name=\"Message\",\n            name=\"message\",\n            method=\"message_response\",\n        ),\n    ]\n\n    def _build_source(self, id_: str | None, display_name: str | None, source: str | None) -> Source:\n        source_dict = {}\n        if id_:\n            source_dict[\"id\"] = id_\n        if display_name:\n            source_dict[\"display_name\"] = display_name\n        if source:\n            # Handle case where source is a ChatOpenAI object\n            if hasattr(source, \"model_name\"):\n                source_dict[\"source\"] = source.model_name\n            elif hasattr(source, \"model\"):\n                source_dict[\"source\"] = str(source.model)\n            else:\n                source_dict[\"source\"] = str(source)\n        return Source(**source_dict)\n\n    async def message_response(self) -> Message:\n        # First convert the input to string if needed\n        text = self.convert_to_string()\n\n        # Get source properties\n        source, icon, display_name, source_id = self.get_properties_from_source_component()\n        background_color = self.background_color\n        text_color = self.text_color\n        if self.chat_icon:\n            icon = self.chat_icon\n\n        # Create or use existing Message object\n        if isinstance(self.input_value, Message):\n            message = self.input_value\n            # Update message properties\n            message.text = text\n        else:\n            message = Message(text=text)\n\n        # Set message properties\n        message.sender = self.sender\n        message.sender_name = self.sender_name\n        message.session_id = self.session_id\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\n        message.properties.source = self._build_source(source_id, display_name, source)\n        message.properties.icon = icon\n        message.properties.background_color = background_color\n        message.properties.text_color = text_color\n\n        # Store message if needed\n        if self.session_id and self.should_store_message:\n            stored_message = await self.send_message(message)\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n\n    def _validate_input(self) -> None:\n        \"\"\"Validate the input data and raise ValueError if invalid.\"\"\"\n        if self.input_value is None:\n            msg = \"Input data cannot be None\"\n            raise ValueError(msg)\n        if isinstance(self.input_value, list) and not all(\n            isinstance(item, Message | Data | DataFrame | str) for item in self.input_value\n        ):\n            invalid_types = [\n                type(item).__name__\n                for item in self.input_value\n                if not isinstance(item, Message | Data | DataFrame | str)\n            ]\n            msg = f\"Expected Data or DataFrame or Message or str, got {invalid_types}\"\n            raise TypeError(msg)\n        if not isinstance(\n            self.input_value,\n            Message | Data | DataFrame | str | list | Generator | type(None),\n        ):\n            type_name = type(self.input_value).__name__\n            msg = f\"Expected Data or DataFrame or Message or str, Generator or None, got {type_name}\"\n            raise TypeError(msg)\n\n    def _serialize_data(self, data: Data) -> str:\n        \"\"\"Serialize Data object to JSON string.\"\"\"\n        # Convert data.data to JSON-serializable format\n        serializable_data = jsonable_encoder(data.data)\n        # Serialize with orjson, enabling pretty printing with indentation\n        json_bytes = orjson.dumps(serializable_data, option=orjson.OPT_INDENT_2)\n        # Convert bytes to string and wrap in Markdown code blocks\n        return \"```json\\n\" + json_bytes.decode(\"utf-8\") + \"\\n```\"\n\n    def _safe_convert(self, data: Any) -> str:\n        \"\"\"Safely convert input data to string.\"\"\"\n        try:\n            if isinstance(data, str):\n                return data\n            if isinstance(data, Message):\n                return data.get_text()\n            if isinstance(data, Data):\n                return self._serialize_data(data)\n            if isinstance(data, DataFrame):\n                if self.clean_data:\n                    # Remove empty rows\n                    data = data.dropna(how=\"all\")\n                    # Remove empty lines in each cell\n                    data = data.replace(r\"^\\s*$\", \"\", regex=True)\n                    # Replace multiple newlines with a single newline\n                    data = data.replace(r\"\\n+\", \"\\n\", regex=True)\n\n                # Replace pipe characters to avoid markdown table issues\n                processed_data = data.replace(r\"\\|\", r\"\\\\|\", regex=True)\n\n                processed_data = processed_data.map(\n                    lambda x: str(x).replace(\"\\n\", \"<br/>\") if isinstance(x, str) else x\n                )\n\n                return processed_data.to_markdown(index=False)\n            return str(data)\n        except (ValueError, TypeError, AttributeError) as e:\n            msg = f\"Error converting data: {e!s}\"\n            raise ValueError(msg) from e\n\n    def convert_to_string(self) -> str | Generator[Any, None, None]:\n        \"\"\"Convert input data to string with proper error handling.\"\"\"\n        self._validate_input()\n        if isinstance(self.input_value, list):\n            return \"\\n\".join([self._safe_convert(item) for item in self.input_value])\n        if isinstance(self.input_value, Generator):\n            return self.input_value\n        return self._safe_convert(self.input_value)\n"
							},
							"data_template": {
								"_input_type": "MessageTextInput",
								"advanced": true,
								"display_name": "Data Template",
								"dynamic": false,
								"info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "data_template",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": "{text}"
							},
							"input_value": {
								"_input_type": "HandleInput",
								"advanced": false,
								"display_name": "Text",
								"dynamic": false,
								"info": "Message to be passed as output.",
								"input_types": ["Data", "DataFrame", "Message"],
								"list": false,
								"list_add_label": "Add More",
								"name": "input_value",
								"placeholder": "",
								"required": true,
								"show": true,
								"title_case": false,
								"trace_as_metadata": true,
								"type": "other",
								"value": ""
							},
							"sender": {
								"_input_type": "DropdownInput",
								"advanced": true,
								"combobox": false,
								"dialog_inputs": {},
								"display_name": "Sender Type",
								"dynamic": false,
								"info": "Type of sender.",
								"name": "sender",
								"options": ["Machine", "User"],
								"options_metadata": [],
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"toggle": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "str",
								"value": "Machine"
							},
							"sender_name": {
								"_input_type": "MessageTextInput",
								"advanced": true,
								"display_name": "Sender Name",
								"dynamic": false,
								"info": "Name of the sender.",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "sender_name",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": "AI"
							},
							"session_id": {
								"_input_type": "MessageTextInput",
								"advanced": true,
								"display_name": "Session ID",
								"dynamic": false,
								"info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "session_id",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": ""
							},
							"should_store_message": {
								"_input_type": "BoolInput",
								"advanced": true,
								"display_name": "Store Messages",
								"dynamic": false,
								"info": "Store the message in the history.",
								"list": false,
								"list_add_label": "Add More",
								"name": "should_store_message",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "bool",
								"value": true
							},
							"text_color": {
								"_input_type": "MessageTextInput",
								"advanced": true,
								"display_name": "Text Color",
								"dynamic": false,
								"info": "The text color of the name",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "text_color",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": ""
							}
						},
						"tool_mode": false
					},
					"showNode": false,
					"type": "ChatOutput"
				},
				"dragging": false,
				"id": "ChatOutput-2vEXM",
				"measured": {
					"height": 48,
					"width": 192
				},
				"position": {
					"x": 2442.2146202521426,
					"y": 1567.1202188294915
				},
				"selected": false,
				"type": "genericNode"
			},
			{
				"data": {
					"id": "Memory-tvjD7",
					"node": {
						"base_classes": ["Data", "DataFrame", "Message"],
						"beta": false,
						"conditional_paths": [],
						"custom_fields": {},
						"description": "Retrieves stored chat messages from Langflow tables or an external memory.",
						"display_name": "Message History",
						"documentation": "",
						"edited": false,
						"field_order": [
							"memory",
							"sender",
							"sender_name",
							"n_messages",
							"session_id",
							"order",
							"template"
						],
						"frozen": false,
						"icon": "message-square-more",
						"legacy": false,
						"lf_version": "1.4.2.dev8",
						"metadata": {},
						"minimized": false,
						"output_types": [],
						"outputs": [
							{
								"allows_loop": false,
								"cache": true,
								"display_name": "Data",
								"method": "retrieve_messages",
								"name": "messages",
								"tool_mode": true,
								"types": ["Data"],
								"value": "__UNDEFINED__"
							},
							{
								"allows_loop": false,
								"cache": true,
								"display_name": "Message",
								"hidden": false,
								"method": "retrieve_messages_as_text",
								"name": "messages_text",
								"selected": "Message",
								"tool_mode": true,
								"types": ["Message"],
								"value": "__UNDEFINED__"
							},
							{
								"allows_loop": false,
								"cache": true,
								"display_name": "DataFrame",
								"method": "as_dataframe",
								"name": "dataframe",
								"tool_mode": true,
								"types": ["DataFrame"],
								"value": "__UNDEFINED__"
							}
						],
						"pinned": false,
						"template": {
							"_type": "Component",
							"code": {
								"advanced": true,
								"dynamic": true,
								"fileTypes": [],
								"file_path": "",
								"info": "",
								"list": false,
								"load_from_db": false,
								"multiline": true,
								"name": "code",
								"password": false,
								"placeholder": "",
								"required": true,
								"show": true,
								"title_case": false,
								"type": "code",
								"value": "from typing import cast\n\nfrom langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.inputs import HandleInput\nfrom langflow.io import DropdownInput, IntInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import aget_messages\nfrom langflow.schema import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER\n\n\nclass MemoryComponent(Component):\n    display_name = \"Message History\"\n    description = \"Retrieves stored chat messages from Langflow tables or an external memory.\"\n    icon = \"message-square-more\"\n    name = \"Memory\"\n\n    inputs = [\n        HandleInput(\n            name=\"memory\",\n            display_name=\"External Memory\",\n            input_types=[\"Memory\"],\n            info=\"Retrieve messages from an external memory. If empty, it will use the Langflow tables.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, \"Machine and User\"],\n            value=\"Machine and User\",\n            info=\"Filter by sender type.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Filter by sender name.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"n_messages\",\n            display_name=\"Number of Messages\",\n            value=100,\n            info=\"Number of messages to retrieve.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"order\",\n            display_name=\"Order\",\n            options=[\"Ascending\", \"Descending\"],\n            value=\"Ascending\",\n            info=\"Order of the messages.\",\n            advanced=True,\n            tool_mode=True,\n        ),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. \"\n            \"It can contain the keys {text}, {sender} or any other key in the message data.\",\n            value=\"{sender_name}: {text}\",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"messages\", method=\"retrieve_messages\"),\n        Output(display_name=\"Message\", name=\"messages_text\", method=\"retrieve_messages_as_text\"),\n        Output(display_name=\"DataFrame\", name=\"dataframe\", method=\"as_dataframe\"),\n    ]\n\n    async def retrieve_messages(self) -> Data:\n        sender = self.sender\n        sender_name = self.sender_name\n        session_id = self.session_id\n        n_messages = self.n_messages\n        order = \"DESC\" if self.order == \"Descending\" else \"ASC\"\n\n        if sender == \"Machine and User\":\n            sender = None\n\n        if self.memory and not hasattr(self.memory, \"aget_messages\"):\n            memory_name = type(self.memory).__name__\n            err_msg = f\"External Memory object ({memory_name}) must have 'aget_messages' method.\"\n            raise AttributeError(err_msg)\n        # Check if n_messages is None or 0\n        if n_messages == 0:\n            stored = []\n        elif self.memory:\n            # override session_id\n            self.memory.session_id = session_id\n\n            stored = await self.memory.aget_messages()\n            # langchain memories are supposed to return messages in ascending order\n            if order == \"DESC\":\n                stored = stored[::-1]\n            if n_messages:\n                stored = stored[:n_messages]\n            stored = [Message.from_lc_message(m) for m in stored]\n            if sender:\n                expected_type = MESSAGE_SENDER_AI if sender == MESSAGE_SENDER_AI else MESSAGE_SENDER_USER\n                stored = [m for m in stored if m.type == expected_type]\n        else:\n            stored = await aget_messages(\n                sender=sender,\n                sender_name=sender_name,\n                session_id=session_id,\n                limit=n_messages,\n                order=order,\n            )\n        self.status = stored\n        return cast(Data, stored)\n\n    async def retrieve_messages_as_text(self) -> Message:\n        stored_text = data_to_text(self.template, await self.retrieve_messages())\n        self.status = stored_text\n        return Message(text=stored_text)\n\n    async def as_dataframe(self) -> DataFrame:\n        \"\"\"Convert the retrieved messages into a DataFrame.\n\n        Returns:\n            DataFrame: A DataFrame containing the message data.\n        \"\"\"\n        messages = await self.retrieve_messages()\n        return DataFrame(messages)\n"
							},
							"memory": {
								"_input_type": "HandleInput",
								"advanced": false,
								"display_name": "External Memory",
								"dynamic": false,
								"info": "Retrieve messages from an external memory. If empty, it will use the Langflow tables.",
								"input_types": ["Memory"],
								"list": false,
								"list_add_label": "Add More",
								"name": "memory",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"trace_as_metadata": true,
								"type": "other",
								"value": ""
							},
							"n_messages": {
								"_input_type": "IntInput",
								"advanced": true,
								"display_name": "Number of Messages",
								"dynamic": false,
								"info": "Number of messages to retrieve.",
								"list": false,
								"list_add_label": "Add More",
								"name": "n_messages",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "int",
								"value": 100
							},
							"order": {
								"_input_type": "DropdownInput",
								"advanced": true,
								"combobox": false,
								"dialog_inputs": {},
								"display_name": "Order",
								"dynamic": false,
								"info": "Order of the messages.",
								"name": "order",
								"options": ["Ascending", "Descending"],
								"options_metadata": [],
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"toggle": false,
								"tool_mode": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": "Ascending"
							},
							"sender": {
								"_input_type": "DropdownInput",
								"advanced": true,
								"combobox": false,
								"dialog_inputs": {},
								"display_name": "Sender Type",
								"dynamic": false,
								"info": "Filter by sender type.",
								"name": "sender",
								"options": ["Machine", "User", "Machine and User"],
								"options_metadata": [],
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"toggle": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "str",
								"value": "Machine and User"
							},
							"sender_name": {
								"_input_type": "MessageTextInput",
								"advanced": true,
								"display_name": "Sender Name",
								"dynamic": false,
								"info": "Filter by sender name.",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "sender_name",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": ""
							},
							"session_id": {
								"_input_type": "MessageTextInput",
								"advanced": true,
								"display_name": "Session ID",
								"dynamic": false,
								"info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "session_id",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": ""
							},
							"template": {
								"_input_type": "MultilineInput",
								"advanced": true,
								"copy_field": false,
								"display_name": "Template",
								"dynamic": false,
								"info": "The template to use for formatting the data. It can contain the keys {text}, {sender} or any other key in the message data.",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"multiline": true,
								"name": "template",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": "{sender_name}: {text}"
							}
						},
						"tool_mode": false
					},
					"showNode": true,
					"type": "Memory"
				},
				"dragging": false,
				"id": "Memory-tvjD7",
				"measured": {
					"height": 187,
					"width": 320
				},
				"position": {
					"x": 644.0428425709895,
					"y": 1591.4330676719069
				},
				"selected": false,
				"type": "genericNode"
			},
			{
				"data": {
					"id": "Prompt-FSzc4",
					"node": {
						"base_classes": ["Message"],
						"beta": false,
						"conditional_paths": [],
						"custom_fields": {
							"template": ["chunks", "message", "history"]
						},
						"description": "Create a prompt template with dynamic variables.",
						"display_name": "Prompt",
						"documentation": "",
						"edited": false,
						"error": null,
						"field_order": ["template", "tool_placeholder"],
						"frozen": false,
						"full_path": null,
						"icon": "prompts",
						"is_composition": null,
						"is_input": null,
						"is_output": null,
						"legacy": false,
						"metadata": {},
						"minimized": false,
						"name": "",
						"output_types": [],
						"outputs": [
							{
								"allows_loop": false,
								"cache": true,
								"display_name": "Prompt Message",
								"hidden": false,
								"method": "build_prompt",
								"name": "prompt",
								"options": null,
								"required_inputs": null,
								"selected": "Message",
								"tool_mode": true,
								"types": ["Message"],
								"value": "__UNDEFINED__"
							}
						],
						"pinned": false,
						"priority": null,
						"template": {
							"_type": "Component",
							"chunks": {
								"advanced": false,
								"display_name": "chunks",
								"dynamic": false,
								"field_type": "str",
								"fileTypes": [],
								"file_path": "",
								"info": "",
								"input_types": ["Message"],
								"list": false,
								"load_from_db": false,
								"multiline": true,
								"name": "chunks",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"type": "str",
								"value": ""
							},
							"code": {
								"advanced": true,
								"dynamic": true,
								"fileTypes": [],
								"file_path": "",
								"info": "",
								"list": false,
								"load_from_db": false,
								"multiline": true,
								"name": "code",
								"password": false,
								"placeholder": "",
								"required": true,
								"show": true,
								"title_case": false,
								"type": "code",
								"value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
							},
							"history": {
								"advanced": false,
								"display_name": "history",
								"dynamic": false,
								"field_type": "str",
								"fileTypes": [],
								"file_path": "",
								"info": "",
								"input_types": ["Message"],
								"list": false,
								"load_from_db": false,
								"multiline": true,
								"name": "history",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"type": "str",
								"value": ""
							},
							"message": {
								"advanced": false,
								"display_name": "message",
								"dynamic": false,
								"field_type": "str",
								"fileTypes": [],
								"file_path": "",
								"info": "",
								"input_types": ["Message"],
								"list": false,
								"load_from_db": false,
								"multiline": true,
								"name": "message",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"type": "str",
								"value": ""
							},
							"template": {
								"_input_type": "PromptInput",
								"advanced": false,
								"display_name": "Template",
								"dynamic": false,
								"info": "",
								"list": false,
								"list_add_label": "Add More",
								"name": "template",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"type": "prompt",
								"value": "As a help desk agent helping the user to create a well defined help desk support ticket for a human operator to follow up on within 24-48 hours, answer the user query by using the context.\n\nContext: {chunks}\nUser: {message}\"\nHistory: {history}\n\nResponses should always begin with either one of two response types:\n\n1. ticket responses: if the current user message and history are sufficient to create a well defined help desk support ticket or if the customer explicitly asks to create a ticket anyways, start the response with the word “ticket:” followed by a well formatted help desk ticket. Well formed tickets will include the user’s name, email, and the most relevant category from the context.\n\nor\n\n2. clarifying responses: if the current user message and history is not sufficient to create a well defined help desk support ticket, start the response with the word “clarify:” followed by a short (approximately one paragraph) request containing guidance for the user to provide more information to help refine and expand on the information needed to create a better ticket. \n\nFor clarifying responses only, respond with a note to the user that they can always respond with “create a ticket anyway” to create a support ticket immediately and not answer any more questions."
							},
							"tool_placeholder": {
								"_input_type": "MessageTextInput",
								"advanced": true,
								"display_name": "Tool Placeholder",
								"dynamic": false,
								"info": "A placeholder input for tool mode.",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "tool_placeholder",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": true,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": ""
							}
						},
						"tool_mode": false
					},
					"showNode": true,
					"type": "Prompt"
				},
				"dragging": false,
				"id": "Prompt-FSzc4",
				"measured": {
					"height": 529,
					"width": 320
				},
				"position": {
					"x": 1482.1291386436187,
					"y": 1420.0175938337184
				},
				"selected": true,
				"type": "genericNode"
			},
			{
				"data": {
					"id": "SplitText-s8WZO",
					"node": {
						"base_classes": ["Data", "DataFrame"],
						"beta": false,
						"conditional_paths": [],
						"custom_fields": {},
						"description": "Split text into chunks based on specified criteria.",
						"display_name": "Split Text",
						"documentation": "",
						"edited": false,
						"field_order": [
							"data_inputs",
							"chunk_overlap",
							"chunk_size",
							"separator",
							"text_key",
							"keep_separator"
						],
						"frozen": false,
						"icon": "scissors-line-dashed",
						"legacy": false,
						"lf_version": "1.4.2.dev8",
						"metadata": {},
						"minimized": false,
						"output_types": [],
						"outputs": [
							{
								"allows_loop": false,
								"cache": true,
								"display_name": "Chunks",
								"hidden": false,
								"method": "split_text",
								"name": "chunks",
								"selected": "Data",
								"tool_mode": true,
								"types": ["Data"],
								"value": "__UNDEFINED__"
							},
							{
								"allows_loop": false,
								"cache": true,
								"display_name": "DataFrame",
								"method": "as_dataframe",
								"name": "dataframe",
								"selected": "DataFrame",
								"tool_mode": true,
								"types": ["DataFrame"],
								"value": "__UNDEFINED__"
							}
						],
						"pinned": false,
						"template": {
							"_type": "Component",
							"chunk_overlap": {
								"_input_type": "IntInput",
								"advanced": false,
								"display_name": "Chunk Overlap",
								"dynamic": false,
								"info": "Number of characters to overlap between chunks.",
								"list": false,
								"list_add_label": "Add More",
								"name": "chunk_overlap",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "int",
								"value": 200
							},
							"chunk_size": {
								"_input_type": "IntInput",
								"advanced": false,
								"display_name": "Chunk Size",
								"dynamic": false,
								"info": "The maximum length of each chunk. Text is first split by separator, then chunks are merged up to this size. Individual splits larger than this won't be further divided.",
								"list": false,
								"list_add_label": "Add More",
								"name": "chunk_size",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "int",
								"value": 1000
							},
							"code": {
								"advanced": true,
								"dynamic": true,
								"fileTypes": [],
								"file_path": "",
								"info": "",
								"list": false,
								"load_from_db": false,
								"multiline": true,
								"name": "code",
								"password": false,
								"placeholder": "",
								"required": true,
								"show": true,
								"title_case": false,
								"type": "code",
								"value": "from langchain_text_splitters import CharacterTextSplitter\n\nfrom langflow.custom import Component\nfrom langflow.io import DropdownInput, HandleInput, IntInput, MessageTextInput, Output\nfrom langflow.schema import Data, DataFrame\nfrom langflow.utils.util import unescape_string\n\n\nclass SplitTextComponent(Component):\n    display_name: str = \"Split Text\"\n    description: str = \"Split text into chunks based on specified criteria.\"\n    icon = \"scissors-line-dashed\"\n    name = \"SplitText\"\n\n    inputs = [\n        HandleInput(\n            name=\"data_inputs\",\n            display_name=\"Data or DataFrame\",\n            info=\"The data with texts to split in chunks.\",\n            input_types=[\"Data\", \"DataFrame\"],\n            required=True,\n        ),\n        IntInput(\n            name=\"chunk_overlap\",\n            display_name=\"Chunk Overlap\",\n            info=\"Number of characters to overlap between chunks.\",\n            value=200,\n        ),\n        IntInput(\n            name=\"chunk_size\",\n            display_name=\"Chunk Size\",\n            info=(\n                \"The maximum length of each chunk. Text is first split by separator, \"\n                \"then chunks are merged up to this size. \"\n                \"Individual splits larger than this won't be further divided.\"\n            ),\n            value=1000,\n        ),\n        MessageTextInput(\n            name=\"separator\",\n            display_name=\"Separator\",\n            info=(\n                \"The character to split on. Use \\\\n for newline. \"\n                \"Examples: \\\\n\\\\n for paragraphs, \\\\n for lines, . for sentences\"\n            ),\n            value=\"\\n\",\n        ),\n        MessageTextInput(\n            name=\"text_key\",\n            display_name=\"Text Key\",\n            info=\"The key to use for the text column.\",\n            value=\"text\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"keep_separator\",\n            display_name=\"Keep Separator\",\n            info=\"Whether to keep the separator in the output chunks and where to place it.\",\n            options=[\"False\", \"True\", \"Start\", \"End\"],\n            value=\"False\",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Chunks\", name=\"chunks\", method=\"split_text\"),\n        Output(display_name=\"DataFrame\", name=\"dataframe\", method=\"as_dataframe\"),\n    ]\n\n    def _docs_to_data(self, docs) -> list[Data]:\n        return [Data(text=doc.page_content, data=doc.metadata) for doc in docs]\n\n    def _fix_separator(self, separator: str) -> str:\n        \"\"\"Fix common separator issues and convert to proper format.\"\"\"\n        if separator == \"/n\":\n            return \"\\n\"\n        if separator == \"/t\":\n            return \"\\t\"\n        return separator\n\n    def split_text_base(self):\n        separator = self._fix_separator(self.separator)\n        separator = unescape_string(separator)\n\n        if isinstance(self.data_inputs, DataFrame):\n            if not len(self.data_inputs):\n                msg = \"DataFrame is empty\"\n                raise TypeError(msg)\n\n            self.data_inputs.text_key = self.text_key\n            try:\n                documents = self.data_inputs.to_lc_documents()\n            except Exception as e:\n                msg = f\"Error converting DataFrame to documents: {e}\"\n                raise TypeError(msg) from e\n        else:\n            if not self.data_inputs:\n                msg = \"No data inputs provided\"\n                raise TypeError(msg)\n\n            documents = []\n            if isinstance(self.data_inputs, Data):\n                self.data_inputs.text_key = self.text_key\n                documents = [self.data_inputs.to_lc_document()]\n            else:\n                try:\n                    documents = [input_.to_lc_document() for input_ in self.data_inputs if isinstance(input_, Data)]\n                    if not documents:\n                        msg = f\"No valid Data inputs found in {type(self.data_inputs)}\"\n                        raise TypeError(msg)\n                except AttributeError as e:\n                    msg = f\"Invalid input type in collection: {e}\"\n                    raise TypeError(msg) from e\n        try:\n            # Convert string 'False'/'True' to boolean\n            keep_sep = self.keep_separator\n            if isinstance(keep_sep, str):\n                if keep_sep.lower() == \"false\":\n                    keep_sep = False\n                elif keep_sep.lower() == \"true\":\n                    keep_sep = True\n                # 'start' and 'end' are kept as strings\n\n            splitter = CharacterTextSplitter(\n                chunk_overlap=self.chunk_overlap,\n                chunk_size=self.chunk_size,\n                separator=separator,\n                keep_separator=keep_sep,\n            )\n            return splitter.split_documents(documents)\n        except Exception as e:\n            msg = f\"Error splitting text: {e}\"\n            raise TypeError(msg) from e\n\n    def split_text(self) -> list[Data]:\n        return self._docs_to_data(self.split_text_base())\n\n    def as_dataframe(self) -> DataFrame:\n        return DataFrame(self.split_text())\n"
							},
							"data_inputs": {
								"_input_type": "HandleInput",
								"advanced": false,
								"display_name": "Data or DataFrame",
								"dynamic": false,
								"info": "The data with texts to split in chunks.",
								"input_types": ["Data", "DataFrame"],
								"list": false,
								"list_add_label": "Add More",
								"name": "data_inputs",
								"placeholder": "",
								"required": true,
								"show": true,
								"title_case": false,
								"trace_as_metadata": true,
								"type": "other",
								"value": ""
							},
							"keep_separator": {
								"_input_type": "DropdownInput",
								"advanced": true,
								"combobox": false,
								"dialog_inputs": {},
								"display_name": "Keep Separator",
								"dynamic": false,
								"info": "Whether to keep the separator in the output chunks and where to place it.",
								"name": "keep_separator",
								"options": ["False", "True", "Start", "End"],
								"options_metadata": [],
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"toggle": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "str",
								"value": "False"
							},
							"separator": {
								"_input_type": "MessageTextInput",
								"advanced": false,
								"display_name": "Separator",
								"dynamic": false,
								"info": "The character to split on. Use \\n for newline. Examples: \\n\\n for paragraphs, \\n for lines, . for sentences",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "separator",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": "\n"
							},
							"text_key": {
								"_input_type": "MessageTextInput",
								"advanced": true,
								"display_name": "Text Key",
								"dynamic": false,
								"info": "The key to use for the text column.",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "text_key",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": "text"
							}
						},
						"tool_mode": false
					},
					"showNode": true,
					"type": "SplitText"
				},
				"dragging": false,
				"id": "SplitText-s8WZO",
				"measured": {
					"height": 417,
					"width": 320
				},
				"position": {
					"x": 132.6270391547173,
					"y": 789.7510798449571
				},
				"selected": false,
				"type": "genericNode"
			},
			{
				"data": {
					"id": "Chroma-3QY7S",
					"node": {
						"base_classes": ["Data", "DataFrame"],
						"beta": false,
						"conditional_paths": [],
						"custom_fields": {},
						"description": "Chroma Vector Store with search capabilities",
						"display_name": "Chroma DB",
						"documentation": "",
						"edited": false,
						"field_order": [
							"collection_name",
							"persist_directory",
							"ingest_data",
							"search_query",
							"should_cache_vector_store",
							"embedding",
							"chroma_server_cors_allow_origins",
							"chroma_server_host",
							"chroma_server_http_port",
							"chroma_server_grpc_port",
							"chroma_server_ssl_enabled",
							"allow_duplicates",
							"search_type",
							"number_of_results",
							"limit"
						],
						"frozen": false,
						"icon": "Chroma",
						"legacy": false,
						"lf_version": "1.4.2.dev8",
						"metadata": {},
						"minimized": false,
						"output_types": [],
						"outputs": [
							{
								"allows_loop": false,
								"cache": true,
								"display_name": "Search Results",
								"hidden": false,
								"method": "search_documents",
								"name": "search_results",
								"required_inputs": [],
								"selected": "Data",
								"tool_mode": true,
								"types": ["Data"],
								"value": "__UNDEFINED__"
							},
							{
								"allows_loop": false,
								"cache": true,
								"display_name": "DataFrame",
								"method": "as_dataframe",
								"name": "dataframe",
								"required_inputs": [],
								"selected": "DataFrame",
								"tool_mode": true,
								"types": ["DataFrame"],
								"value": "__UNDEFINED__"
							}
						],
						"pinned": false,
						"template": {
							"_type": "Component",
							"allow_duplicates": {
								"_input_type": "BoolInput",
								"advanced": true,
								"display_name": "Allow Duplicates",
								"dynamic": false,
								"info": "If false, will not add documents that are already in the Vector Store.",
								"list": false,
								"list_add_label": "Add More",
								"name": "allow_duplicates",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "bool",
								"value": false
							},
							"chroma_server_cors_allow_origins": {
								"_input_type": "StrInput",
								"advanced": true,
								"display_name": "Server CORS Allow Origins",
								"dynamic": false,
								"info": "",
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "chroma_server_cors_allow_origins",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "str",
								"value": ""
							},
							"chroma_server_grpc_port": {
								"_input_type": "IntInput",
								"advanced": true,
								"display_name": "Server gRPC Port",
								"dynamic": false,
								"info": "",
								"list": false,
								"list_add_label": "Add More",
								"name": "chroma_server_grpc_port",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "int",
								"value": ""
							},
							"chroma_server_host": {
								"_input_type": "StrInput",
								"advanced": true,
								"display_name": "Server Host",
								"dynamic": false,
								"info": "",
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "chroma_server_host",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "str",
								"value": ""
							},
							"chroma_server_http_port": {
								"_input_type": "IntInput",
								"advanced": true,
								"display_name": "Server HTTP Port",
								"dynamic": false,
								"info": "",
								"list": false,
								"list_add_label": "Add More",
								"name": "chroma_server_http_port",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "int",
								"value": ""
							},
							"chroma_server_ssl_enabled": {
								"_input_type": "BoolInput",
								"advanced": true,
								"display_name": "Server SSL Enabled",
								"dynamic": false,
								"info": "",
								"list": false,
								"list_add_label": "Add More",
								"name": "chroma_server_ssl_enabled",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "bool",
								"value": false
							},
							"code": {
								"advanced": true,
								"dynamic": true,
								"fileTypes": [],
								"file_path": "",
								"info": "",
								"list": false,
								"load_from_db": false,
								"multiline": true,
								"name": "code",
								"password": false,
								"placeholder": "",
								"required": true,
								"show": true,
								"title_case": false,
								"type": "code",
								"value": "from copy import deepcopy\n\nfrom chromadb.config import Settings\nfrom langchain_chroma import Chroma\nfrom typing_extensions import override\n\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.base.vectorstores.utils import chroma_collection_to_data\nfrom langflow.io import BoolInput, DropdownInput, HandleInput, IntInput, StrInput\nfrom langflow.schema import Data, DataFrame\n\n\nclass ChromaVectorStoreComponent(LCVectorStoreComponent):\n    \"\"\"Chroma Vector Store with search capabilities.\"\"\"\n\n    display_name: str = \"Chroma DB\"\n    description: str = \"Chroma Vector Store with search capabilities\"\n    name = \"Chroma\"\n    icon = \"Chroma\"\n\n    inputs = [\n        StrInput(\n            name=\"collection_name\",\n            display_name=\"Collection Name\",\n            value=\"langflow\",\n        ),\n        StrInput(\n            name=\"persist_directory\",\n            display_name=\"Persist Directory\",\n        ),\n        *LCVectorStoreComponent.inputs,\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        StrInput(\n            name=\"chroma_server_cors_allow_origins\",\n            display_name=\"Server CORS Allow Origins\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"chroma_server_host\",\n            display_name=\"Server Host\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"chroma_server_http_port\",\n            display_name=\"Server HTTP Port\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"chroma_server_grpc_port\",\n            display_name=\"Server gRPC Port\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"chroma_server_ssl_enabled\",\n            display_name=\"Server SSL Enabled\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"allow_duplicates\",\n            display_name=\"Allow Duplicates\",\n            advanced=True,\n            info=\"If false, will not add documents that are already in the Vector Store.\",\n        ),\n        DropdownInput(\n            name=\"search_type\",\n            display_name=\"Search Type\",\n            options=[\"Similarity\", \"MMR\"],\n            value=\"Similarity\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            advanced=True,\n            value=10,\n        ),\n        IntInput(\n            name=\"limit\",\n            display_name=\"Limit\",\n            advanced=True,\n            info=\"Limit the number of records to compare when Allow Duplicates is False.\",\n        ),\n    ]\n\n    @override\n    @check_cached_vector_store\n    def build_vector_store(self) -> Chroma:\n        \"\"\"Builds the Chroma object.\"\"\"\n        try:\n            from chromadb import Client\n            from langchain_chroma import Chroma\n        except ImportError as e:\n            msg = \"Could not import Chroma integration package. Please install it with `pip install langchain-chroma`.\"\n            raise ImportError(msg) from e\n        # Chroma settings\n        chroma_settings = None\n        client = None\n        if self.chroma_server_host:\n            chroma_settings = Settings(\n                chroma_server_cors_allow_origins=self.chroma_server_cors_allow_origins or [],\n                chroma_server_host=self.chroma_server_host,\n                chroma_server_http_port=self.chroma_server_http_port or None,\n                chroma_server_grpc_port=self.chroma_server_grpc_port or None,\n                chroma_server_ssl_enabled=self.chroma_server_ssl_enabled,\n            )\n            client = Client(settings=chroma_settings)\n\n        # Check persist_directory and expand it if it is a relative path\n        persist_directory = self.resolve_path(self.persist_directory) if self.persist_directory is not None else None\n\n        chroma = Chroma(\n            persist_directory=persist_directory,\n            client=client,\n            embedding_function=self.embedding,\n            collection_name=self.collection_name,\n        )\n\n        self._add_documents_to_vector_store(chroma)\n        self.status = chroma_collection_to_data(chroma.get(limit=self.limit))\n        return chroma\n\n    def _add_documents_to_vector_store(self, vector_store: \"Chroma\") -> None:\n        \"\"\"Adds documents to the Vector Store.\"\"\"\n        ingest_data: list | Data | DataFrame = self.ingest_data\n        if not ingest_data:\n            self.status = \"\"\n            return\n\n        # Convert DataFrame to Data if needed using parent's method\n        ingest_data = self._prepare_ingest_data()\n\n        stored_documents_without_id = []\n        if self.allow_duplicates:\n            stored_data = []\n        else:\n            stored_data = chroma_collection_to_data(vector_store.get(limit=self.limit))\n            for value in deepcopy(stored_data):\n                del value.id\n                stored_documents_without_id.append(value)\n\n        documents = []\n        for _input in ingest_data or []:\n            if isinstance(_input, Data):\n                if _input not in stored_documents_without_id:\n                    documents.append(_input.to_lc_document())\n            else:\n                msg = \"Vector Store Inputs must be Data objects.\"\n                raise TypeError(msg)\n\n        if documents and self.embedding is not None:\n            self.log(f\"Adding {len(documents)} documents to the Vector Store.\")\n            vector_store.add_documents(documents)\n        else:\n            self.log(\"No documents to add to the Vector Store.\")\n"
							},
							"collection_name": {
								"_input_type": "StrInput",
								"advanced": false,
								"display_name": "Collection Name",
								"dynamic": false,
								"info": "",
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "collection_name",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "str",
								"value": "langflow"
							},
							"embedding": {
								"_input_type": "HandleInput",
								"advanced": false,
								"display_name": "Embedding",
								"dynamic": false,
								"info": "",
								"input_types": ["Embeddings"],
								"list": false,
								"list_add_label": "Add More",
								"name": "embedding",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"trace_as_metadata": true,
								"type": "other",
								"value": ""
							},
							"ingest_data": {
								"_input_type": "HandleInput",
								"advanced": false,
								"display_name": "Ingest Data",
								"dynamic": false,
								"info": "",
								"input_types": ["Data", "DataFrame"],
								"list": true,
								"list_add_label": "Add More",
								"name": "ingest_data",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"trace_as_metadata": true,
								"type": "other",
								"value": ""
							},
							"limit": {
								"_input_type": "IntInput",
								"advanced": true,
								"display_name": "Limit",
								"dynamic": false,
								"info": "Limit the number of records to compare when Allow Duplicates is False.",
								"list": false,
								"list_add_label": "Add More",
								"name": "limit",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "int",
								"value": ""
							},
							"number_of_results": {
								"_input_type": "IntInput",
								"advanced": true,
								"display_name": "Number of Results",
								"dynamic": false,
								"info": "Number of results to return.",
								"list": false,
								"list_add_label": "Add More",
								"name": "number_of_results",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "int",
								"value": 10
							},
							"persist_directory": {
								"_input_type": "StrInput",
								"advanced": false,
								"display_name": "Persist Directory",
								"dynamic": false,
								"info": "",
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "persist_directory",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "str",
								"value": ""
							},
							"search_query": {
								"_input_type": "QueryInput",
								"advanced": false,
								"display_name": "Search Query",
								"dynamic": false,
								"info": "Enter a query to run a similarity search.",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "search_query",
								"placeholder": "Enter a query...",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": true,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "query",
								"value": ""
							},
							"search_type": {
								"_input_type": "DropdownInput",
								"advanced": true,
								"combobox": false,
								"dialog_inputs": {},
								"display_name": "Search Type",
								"dynamic": false,
								"info": "",
								"name": "search_type",
								"options": ["Similarity", "MMR"],
								"options_metadata": [],
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"toggle": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "str",
								"value": "Similarity"
							},
							"should_cache_vector_store": {
								"_input_type": "BoolInput",
								"advanced": true,
								"display_name": "Cache Vector Store",
								"dynamic": false,
								"info": "If True, the vector store will be cached for the current build of the component. This is useful for components that have multiple output methods and want to share the same vector store.",
								"list": false,
								"list_add_label": "Add More",
								"name": "should_cache_vector_store",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "bool",
								"value": true
							}
						},
						"tool_mode": false
					},
					"showNode": true,
					"type": "Chroma"
				},
				"dragging": false,
				"id": "Chroma-3QY7S",
				"measured": {
					"height": 461,
					"width": 320
				},
				"position": {
					"x": 701.8717300539047,
					"y": 791.7604777018753
				},
				"selected": false,
				"type": "genericNode"
			},
			{
				"data": {
					"id": "OllamaEmbeddings-DxU0f",
					"node": {
						"base_classes": ["Embeddings"],
						"beta": false,
						"conditional_paths": [],
						"custom_fields": {},
						"description": "Generate embeddings using Ollama models.",
						"display_name": "Ollama Embeddings",
						"documentation": "https://python.langchain.com/docs/integrations/text_embedding/ollama",
						"edited": false,
						"field_order": ["model_name", "base_url"],
						"frozen": false,
						"icon": "Ollama",
						"legacy": false,
						"lf_version": "1.4.2.dev8",
						"metadata": {
							"keywords": ["model", "llm", "language model", "large language model"]
						},
						"minimized": false,
						"output_types": [],
						"outputs": [
							{
								"allows_loop": false,
								"cache": true,
								"display_name": "Embeddings",
								"hidden": false,
								"method": "build_embeddings",
								"name": "embeddings",
								"options": null,
								"required_inputs": null,
								"selected": "Embeddings",
								"tool_mode": true,
								"types": ["Embeddings"],
								"value": "__UNDEFINED__"
							}
						],
						"pinned": false,
						"template": {
							"_type": "Component",
							"base_url": {
								"_input_type": "MessageTextInput",
								"advanced": false,
								"display_name": "Ollama Base URL",
								"dynamic": false,
								"info": "",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "base_url",
								"placeholder": "",
								"required": true,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": "http://localhost:11434"
							},
							"code": {
								"advanced": true,
								"dynamic": true,
								"fileTypes": [],
								"file_path": "",
								"info": "",
								"list": false,
								"load_from_db": false,
								"multiline": true,
								"name": "code",
								"password": false,
								"placeholder": "",
								"required": true,
								"show": true,
								"title_case": false,
								"type": "code",
								"value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_ollama import OllamaEmbeddings\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.ollama_constants import OLLAMA_EMBEDDING_MODELS, URL_LIST\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import DropdownInput, MessageTextInput, Output\n\nHTTP_STATUS_OK = 200\n\n\nclass OllamaEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Ollama Embeddings\"\n    description: str = \"Generate embeddings using Ollama models.\"\n    documentation = \"https://python.langchain.com/docs/integrations/text_embedding/ollama\"\n    icon = \"Ollama\"\n    name = \"OllamaEmbeddings\"\n\n    inputs = [\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Ollama Model\",\n            value=\"\",\n            options=[],\n            real_time_refresh=True,\n            refresh_button=True,\n            combobox=True,\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"Ollama Base URL\",\n            value=\"\",\n            required=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            output = OllamaEmbeddings(model=self.model_name, base_url=self.base_url)\n        except Exception as e:\n            msg = (\n                \"Unable to connect to the Ollama API. \",\n                \"Please verify the base URL, ensure the relevant Ollama model is pulled, and try again.\",\n            )\n            raise ValueError(msg) from e\n        return output\n\n    async def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name in {\"base_url\", \"model_name\"} and not await self.is_valid_ollama_url(field_value):\n            # Check if any URL in the list is valid\n            valid_url = \"\"\n            for url in URL_LIST:\n                if await self.is_valid_ollama_url(url):\n                    valid_url = url\n                    break\n            build_config[\"base_url\"][\"value\"] = valid_url\n        if field_name in {\"model_name\", \"base_url\", \"tool_model_enabled\"}:\n            if await self.is_valid_ollama_url(self.base_url):\n                build_config[\"model_name\"][\"options\"] = await self.get_model(self.base_url)\n            elif await self.is_valid_ollama_url(build_config[\"base_url\"].get(\"value\", \"\")):\n                build_config[\"model_name\"][\"options\"] = await self.get_model(build_config[\"base_url\"].get(\"value\", \"\"))\n            else:\n                build_config[\"model_name\"][\"options\"] = []\n\n        return build_config\n\n    async def get_model(self, base_url_value: str) -> list[str]:\n        \"\"\"Get the model names from Ollama.\"\"\"\n        model_ids = []\n        try:\n            url = urljoin(base_url_value, \"/api/tags\")\n            async with httpx.AsyncClient() as client:\n                response = await client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n            model_ids = [model[\"name\"] for model in data.get(\"models\", [])]\n            # this to ensure that not embedding models are included.\n            # not even the base models since models can have 1b 2b etc\n            # handles cases when embeddings models have tags like :latest - etc.\n            model_ids = [\n                model\n                for model in model_ids\n                if any(model.startswith(f\"{embedding_model}\") for embedding_model in OLLAMA_EMBEDDING_MODELS)\n            ]\n\n        except (ImportError, ValueError, httpx.RequestError) as e:\n            msg = \"Could not get model names from Ollama.\"\n            raise ValueError(msg) from e\n\n        return model_ids\n\n    async def is_valid_ollama_url(self, url: str) -> bool:\n        try:\n            async with httpx.AsyncClient() as client:\n                return (await client.get(f\"{url}/api/tags\")).status_code == HTTP_STATUS_OK\n        except httpx.RequestError:\n            return False\n"
							},
							"model_name": {
								"_input_type": "DropdownInput",
								"advanced": false,
								"combobox": true,
								"dialog_inputs": {},
								"display_name": "Ollama Model",
								"dynamic": false,
								"info": "",
								"name": "model_name",
								"options": ["all-minilm:latest"],
								"options_metadata": [],
								"placeholder": "",
								"real_time_refresh": true,
								"refresh_button": true,
								"required": true,
								"show": true,
								"title_case": false,
								"toggle": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "str",
								"value": "all-minilm:latest"
							}
						},
						"tool_mode": false
					},
					"showNode": true,
					"type": "OllamaEmbeddings"
				},
				"dragging": false,
				"id": "OllamaEmbeddings-DxU0f",
				"measured": {
					"height": 291,
					"width": 320
				},
				"position": {
					"x": 283.6636806340165,
					"y": 1371.096172175926
				},
				"selected": false,
				"type": "genericNode"
			},
			{
				"data": {
					"id": "ParserComponent-NWgqo",
					"node": {
						"base_classes": ["Message"],
						"beta": false,
						"conditional_paths": [],
						"custom_fields": {},
						"description": "Format a DataFrame or Data object into text using a template. Enable 'Stringify' to convert input into a readable string instead.",
						"display_name": "Parser",
						"documentation": "",
						"edited": false,
						"field_order": ["mode", "pattern", "input_data", "sep"],
						"frozen": false,
						"icon": "braces",
						"legacy": false,
						"lf_version": "1.4.2.dev8",
						"metadata": {},
						"minimized": false,
						"output_types": [],
						"outputs": [
							{
								"allows_loop": false,
								"cache": true,
								"display_name": "Parsed Text",
								"hidden": false,
								"method": "parse_combined_text",
								"name": "parsed_text",
								"options": null,
								"required_inputs": null,
								"selected": "Message",
								"tool_mode": true,
								"types": ["Message"],
								"value": "__UNDEFINED__"
							}
						],
						"pinned": false,
						"template": {
							"_type": "Component",
							"clean_data": {
								"_input_type": "BoolInput",
								"advanced": true,
								"display_name": "Clean Data",
								"dynamic": false,
								"info": "Enable to clean the data by removing empty rows and lines in each cell of the DataFrame/ Data object.",
								"list": false,
								"list_add_label": "Add More",
								"name": "clean_data",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "bool",
								"value": true
							},
							"code": {
								"advanced": true,
								"dynamic": true,
								"fileTypes": [],
								"file_path": "",
								"info": "",
								"list": false,
								"load_from_db": false,
								"multiline": true,
								"name": "code",
								"password": false,
								"placeholder": "",
								"required": true,
								"show": true,
								"title_case": false,
								"type": "code",
								"value": "import json\nfrom typing import Any\n\nfrom langflow.custom import Component\nfrom langflow.io import (\n    BoolInput,\n    HandleInput,\n    MessageTextInput,\n    MultilineInput,\n    Output,\n    TabInput,\n)\nfrom langflow.schema import Data, DataFrame\nfrom langflow.schema.message import Message\n\n\nclass ParserComponent(Component):\n    display_name = \"Parser\"\n    description = (\n        \"Format a DataFrame or Data object into text using a template. \"\n        \"Enable 'Stringify' to convert input into a readable string instead.\"\n    )\n    icon = \"braces\"\n\n    inputs = [\n        TabInput(\n            name=\"mode\",\n            display_name=\"Mode\",\n            options=[\"Parser\", \"Stringify\"],\n            value=\"Parser\",\n            info=\"Convert into raw string instead of using a template.\",\n            real_time_refresh=True,\n        ),\n        MultilineInput(\n            name=\"pattern\",\n            display_name=\"Template\",\n            info=(\n                \"Use variables within curly brackets to extract column values for DataFrames \"\n                \"or key values for Data.\"\n                \"For example: `Name: {Name}, Age: {Age}, Country: {Country}`\"\n            ),\n            value=\"Text: {text}\",  # Example default\n            dynamic=True,\n            show=True,\n            required=True,\n        ),\n        HandleInput(\n            name=\"input_data\",\n            display_name=\"Data or DataFrame\",\n            input_types=[\"DataFrame\", \"Data\"],\n            info=\"Accepts either a DataFrame or a Data object.\",\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"sep\",\n            display_name=\"Separator\",\n            advanced=True,\n            value=\"\\n\",\n            info=\"String used to separate rows/items.\",\n        ),\n    ]\n\n    outputs = [\n        Output(\n            display_name=\"Parsed Text\",\n            name=\"parsed_text\",\n            info=\"Formatted text output.\",\n            method=\"parse_combined_text\",\n        ),\n    ]\n\n    def update_build_config(self, build_config, field_value, field_name=None):\n        \"\"\"Dynamically hide/show `template` and enforce requirement based on `stringify`.\"\"\"\n        if field_name == \"mode\":\n            build_config[\"pattern\"][\"show\"] = self.mode == \"Parser\"\n            build_config[\"pattern\"][\"required\"] = self.mode == \"Parser\"\n            if field_value:\n                clean_data = BoolInput(\n                    name=\"clean_data\",\n                    display_name=\"Clean Data\",\n                    info=(\n                        \"Enable to clean the data by removing empty rows and lines \"\n                        \"in each cell of the DataFrame/ Data object.\"\n                    ),\n                    value=True,\n                    advanced=True,\n                    required=False,\n                )\n                build_config[\"clean_data\"] = clean_data.to_dict()\n            else:\n                build_config.pop(\"clean_data\", None)\n\n        return build_config\n\n    def _clean_args(self):\n        \"\"\"Prepare arguments based on input type.\"\"\"\n        input_data = self.input_data\n\n        match input_data:\n            case list() if all(isinstance(item, Data) for item in input_data):\n                msg = \"List of Data objects is not supported.\"\n                raise ValueError(msg)\n            case DataFrame():\n                return input_data, None\n            case Data():\n                return None, input_data\n            case dict() if \"data\" in input_data:\n                try:\n                    if \"columns\" in input_data:  # Likely a DataFrame\n                        return DataFrame.from_dict(input_data), None\n                    # Likely a Data object\n                    return None, Data(**input_data)\n                except (TypeError, ValueError, KeyError) as e:\n                    msg = f\"Invalid structured input provided: {e!s}\"\n                    raise ValueError(msg) from e\n            case _:\n                msg = f\"Unsupported input type: {type(input_data)}. Expected DataFrame or Data.\"\n                raise ValueError(msg)\n\n    def parse_combined_text(self) -> Message:\n        \"\"\"Parse all rows/items into a single text or convert input to string if `stringify` is enabled.\"\"\"\n        # Early return for stringify option\n        if self.mode == \"Stringify\":\n            return self.convert_to_string()\n\n        df, data = self._clean_args()\n\n        lines = []\n        if df is not None:\n            for _, row in df.iterrows():\n                formatted_text = self.pattern.format(**row.to_dict())\n                lines.append(formatted_text)\n        elif data is not None:\n            formatted_text = self.pattern.format(**data.data)\n            lines.append(formatted_text)\n\n        combined_text = self.sep.join(lines)\n        self.status = combined_text\n        return Message(text=combined_text)\n\n    def _safe_convert(self, data: Any) -> str:\n        \"\"\"Safely convert input data to string.\"\"\"\n        try:\n            if isinstance(data, str):\n                return data\n            if isinstance(data, Message):\n                return data.get_text()\n            if isinstance(data, Data):\n                return json.dumps(data.data)\n            if isinstance(data, DataFrame):\n                if hasattr(self, \"clean_data\") and self.clean_data:\n                    # Remove empty rows\n                    data = data.dropna(how=\"all\")\n                    # Remove empty lines in each cell\n                    data = data.replace(r\"^\\s*$\", \"\", regex=True)\n                    # Replace multiple newlines with a single newline\n                    data = data.replace(r\"\\n+\", \"\\n\", regex=True)\n                return data.to_markdown(index=False)\n            return str(data)\n        except (ValueError, TypeError, AttributeError) as e:\n            msg = f\"Error converting data: {e!s}\"\n            raise ValueError(msg) from e\n\n    def convert_to_string(self) -> Message:\n        \"\"\"Convert input data to string with proper error handling.\"\"\"\n        result = \"\"\n        if isinstance(self.input_data, list):\n            result = \"\\n\".join([self._safe_convert(item) for item in self.input_data])\n        else:\n            result = self._safe_convert(self.input_data)\n        self.log(f\"Converted to string with length: {len(result)}\")\n\n        message = Message(text=result)\n        self.status = message\n        return message\n"
							},
							"input_data": {
								"_input_type": "HandleInput",
								"advanced": false,
								"display_name": "Data or DataFrame",
								"dynamic": false,
								"info": "Accepts either a DataFrame or a Data object.",
								"input_types": ["DataFrame", "Data"],
								"list": false,
								"list_add_label": "Add More",
								"name": "input_data",
								"placeholder": "",
								"required": true,
								"show": true,
								"title_case": false,
								"trace_as_metadata": true,
								"type": "other",
								"value": ""
							},
							"mode": {
								"_input_type": "TabInput",
								"advanced": false,
								"display_name": "Mode",
								"dynamic": false,
								"info": "Convert into raw string instead of using a template.",
								"name": "mode",
								"options": ["Parser", "Stringify"],
								"placeholder": "",
								"real_time_refresh": true,
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "tab",
								"value": "Stringify"
							},
							"pattern": {
								"_input_type": "MultilineInput",
								"advanced": false,
								"copy_field": false,
								"display_name": "Template",
								"dynamic": true,
								"info": "Use variables within curly brackets to extract column values for DataFrames or key values for Data.For example: `Name: {Name}, Age: {Age}, Country: {Country}`",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"multiline": true,
								"name": "pattern",
								"placeholder": "",
								"required": false,
								"show": false,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": "Text: {text}"
							},
							"sep": {
								"_input_type": "MessageTextInput",
								"advanced": true,
								"display_name": "Separator",
								"dynamic": false,
								"info": "String used to separate rows/items.",
								"input_types": ["Message"],
								"list": false,
								"list_add_label": "Add More",
								"load_from_db": false,
								"name": "sep",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": "\n"
							}
						},
						"tool_mode": false
					},
					"showNode": true,
					"type": "ParserComponent"
				},
				"dragging": false,
				"id": "ParserComponent-NWgqo",
				"measured": {
					"height": 283,
					"width": 320
				},
				"position": {
					"x": 1075.6360302844325,
					"y": 1255.9400391688614
				},
				"selected": false,
				"type": "genericNode"
			},
			{
				"data": {
					"id": "URLComponent-wTaji",
					"node": {
						"base_classes": ["Data", "DataFrame", "Message"],
						"beta": false,
						"conditional_paths": [],
						"custom_fields": {},
						"description": "Load and parse child links from a root URL recursively",
						"display_name": "URL",
						"documentation": "",
						"edited": false,
						"field_order": [
							"urls",
							"max_depth",
							"prevent_outside",
							"use_async",
							"format",
							"timeout",
							"headers"
						],
						"frozen": false,
						"icon": "layout-template",
						"legacy": false,
						"lf_version": "1.4.2.dev8",
						"metadata": {},
						"minimized": false,
						"output_types": [],
						"outputs": [
							{
								"allows_loop": false,
								"cache": true,
								"display_name": "Data",
								"hidden": false,
								"method": "fetch_content",
								"name": "data",
								"selected": "Data",
								"tool_mode": true,
								"types": ["Data"],
								"value": "__UNDEFINED__"
							},
							{
								"allows_loop": false,
								"cache": true,
								"display_name": "Message",
								"method": "fetch_content_text",
								"name": "text",
								"selected": "Message",
								"tool_mode": true,
								"types": ["Message"],
								"value": "__UNDEFINED__"
							},
							{
								"allows_loop": false,
								"cache": true,
								"display_name": "DataFrame",
								"method": "as_dataframe",
								"name": "dataframe",
								"selected": "DataFrame",
								"tool_mode": true,
								"types": ["DataFrame"],
								"value": "__UNDEFINED__"
							}
						],
						"pinned": false,
						"template": {
							"_type": "Component",
							"code": {
								"advanced": true,
								"dynamic": true,
								"fileTypes": [],
								"file_path": "",
								"info": "",
								"list": false,
								"load_from_db": false,
								"multiline": true,
								"name": "code",
								"password": false,
								"placeholder": "",
								"required": true,
								"show": true,
								"title_case": false,
								"type": "code",
								"value": "import re\n\nimport httpx\nfrom bs4 import BeautifulSoup\nfrom langchain_community.document_loaders import RecursiveUrlLoader\nfrom loguru import logger\n\nfrom langflow.custom.custom_component.component import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.inputs.inputs import TableInput\nfrom langflow.io import BoolInput, DropdownInput, IntInput, MessageTextInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.services.deps import get_settings_service\n\n\nclass URLComponent(Component):\n    \"\"\"A component that loads and parses child links from a root URL recursively.\"\"\"\n\n    display_name = \"URL\"\n    description = \"Load and parse child links from a root URL recursively\"\n    icon = \"layout-template\"\n    name = \"URLComponent\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"urls\",\n            display_name=\"URLs\",\n            info=\"Enter one or more URLs to crawl recursively, by clicking the '+' button.\",\n            is_list=True,\n            tool_mode=True,\n            placeholder=\"Enter a URL...\",\n            list_add_label=\"Add URL\",\n        ),\n        IntInput(\n            name=\"max_depth\",\n            display_name=\"Max Depth\",\n            info=(\n                \"Controls how many 'clicks' away from the initial page the crawler will go:\\n\"\n                \"- depth 1: only the initial page\\n\"\n                \"- depth 2: initial page + all pages linked directly from it\\n\"\n                \"- depth 3: initial page + direct links + links found on those direct link pages\\n\"\n                \"Note: This is about link traversal, not URL path depth.\"\n            ),\n            value=1,\n            required=False,\n        ),\n        BoolInput(\n            name=\"prevent_outside\",\n            display_name=\"Prevent Outside\",\n            info=(\n                \"If enabled, only crawls URLs within the same domain as the root URL. \"\n                \"This helps prevent the crawler from going to external websites.\"\n            ),\n            value=True,\n            required=False,\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"use_async\",\n            display_name=\"Use Async\",\n            info=(\n                \"If enabled, uses asynchronous loading which can be significantly faster \"\n                \"but might use more system resources.\"\n            ),\n            value=True,\n            required=False,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"format\",\n            display_name=\"Output Format\",\n            info=\"Output Format. Use 'Text' to extract the text from the HTML or 'HTML' for the raw HTML content.\",\n            options=[\"Text\", \"HTML\"],\n            value=\"Text\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            info=\"Timeout for the request in seconds.\",\n            value=30,\n            required=False,\n            advanced=True,\n        ),\n        TableInput(\n            name=\"headers\",\n            display_name=\"Headers\",\n            info=\"The headers to send with the request\",\n            table_schema=[\n                {\n                    \"name\": \"key\",\n                    \"display_name\": \"Header\",\n                    \"type\": \"str\",\n                    \"description\": \"Header name\",\n                },\n                {\n                    \"name\": \"value\",\n                    \"display_name\": \"Value\",\n                    \"type\": \"str\",\n                    \"description\": \"Header value\",\n                },\n            ],\n            value=[{\"key\": \"User-Agent\", \"value\": get_settings_service().settings.user_agent}],\n            advanced=True,\n            input_types=[\"DataFrame\"],\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"fetch_content\"),\n        Output(display_name=\"Message\", name=\"text\", method=\"fetch_content_text\"),\n        Output(display_name=\"DataFrame\", name=\"dataframe\", method=\"as_dataframe\"),\n    ]\n\n    def validate_url(self, string: str) -> bool:\n        \"\"\"Validates if the given string matches URL pattern.\"\"\"\n        url_regex = re.compile(\n            r\"^(https?:\\/\\/)?\" r\"(www\\.)?\" r\"([a-zA-Z0-9.-]+)\" r\"(\\.[a-zA-Z]{2,})?\" r\"(:\\d+)?\" r\"(\\/[^\\s]*)?$\",\n            re.IGNORECASE,\n        )\n        return bool(url_regex.match(string))\n\n    def ensure_url(self, url: str) -> str:\n        \"\"\"Ensures the given string is a valid URL.\"\"\"\n        if not url.startswith((\"http://\", \"https://\")):\n            url = \"http://\" + url\n\n        if not self.validate_url(url):\n            error_msg = \"Invalid URL - \" + url\n            raise ValueError(error_msg)\n\n        return url\n\n    def fetch_content(self) -> list[Data]:\n        \"\"\"Load documents from the URLs.\"\"\"\n        all_docs = []\n        data = []\n        try:\n            urls = list({self.ensure_url(url.strip()) for url in self.urls if url.strip()})\n\n            no_urls_msg = \"No valid URLs provided.\"\n            if not urls:\n                raise ValueError(no_urls_msg)\n\n            # If there's only one URL, we'll make sure to propagate any errors\n            single_url = len(urls) == 1\n\n            for processed_url in urls:\n                msg = f\"Loading documents from {processed_url}\"\n                logger.info(msg)\n\n                # Create headers dictionary\n                headers_dict = {header[\"key\"]: header[\"value\"] for header in self.headers}\n\n                # Configure RecursiveUrlLoader with httpx-compatible settings\n                extractor = (lambda x: x) if self.format == \"HTML\" else (lambda x: BeautifulSoup(x, \"lxml\").get_text())\n\n                # Modified settings for RecursiveUrlLoader\n                # Note: We need to pass a compatible client or settings to RecursiveUrlLoader\n                # This will depend on how RecursiveUrlLoader is implemented\n                loader = RecursiveUrlLoader(\n                    url=processed_url,\n                    max_depth=self.max_depth,\n                    prevent_outside=self.prevent_outside,\n                    use_async=self.use_async,\n                    continue_on_failure=not single_url,\n                    extractor=extractor,\n                    timeout=self.timeout,\n                    headers=headers_dict,\n                )\n\n                try:\n                    docs = loader.load()\n                    if not docs:\n                        msg = f\"No documents found for {processed_url}\"\n                        logger.warning(msg)\n                        if single_url:\n                            message = f\"No documents found for {processed_url}\"\n                            raise ValueError(message)\n                    else:\n                        msg = f\"Found {len(docs)} documents from {processed_url}\"\n                        logger.info(msg)\n                        all_docs.extend(docs)\n                except (httpx.HTTPError, httpx.RequestError) as e:\n                    msg = f\"Error loading documents from {processed_url}: {e}\"\n                    logger.exception(msg)\n                    if single_url:\n                        raise  # Re-raise the exception if it's the only URL\n                except UnicodeDecodeError as e:\n                    msg = f\"Error decoding content from {processed_url}: {e}\"\n                    logger.error(msg)\n                    if single_url:\n                        raise  # Re-raise the exception if it's the only URL\n                except Exception as e:\n                    msg = f\"Unexpected error loading documents from {processed_url}: {e}\"\n                    logger.exception(msg)\n                    if single_url:\n                        raise  # Re-raise the exception if it's the only URL\n\n            data = [Data(text=doc.page_content, **doc.metadata) for doc in all_docs]\n            self.status = data\n\n        except Exception as e:\n            error_msg = e.message if hasattr(e, \"message\") else e\n            msg = f\"Error loading documents: {error_msg!s}\"\n            logger.exception(msg)\n            raise ValueError(msg) from e\n\n        self.status = data\n        return data\n\n    def fetch_content_text(self) -> Message:\n        \"\"\"Load documents and return their text content.\"\"\"\n        data = self.fetch_content()\n        result_string = data_to_text(\"{text}\", data)\n        self.status = result_string\n        return Message(text=result_string)\n\n    def as_dataframe(self) -> DataFrame:\n        \"\"\"Convert the documents to a DataFrame.\"\"\"\n        data_frame = DataFrame(self.fetch_content())\n        self.status = data_frame\n        return data_frame\n"
							},
							"format": {
								"_input_type": "DropdownInput",
								"advanced": true,
								"combobox": false,
								"dialog_inputs": {},
								"display_name": "Output Format",
								"dynamic": false,
								"info": "Output Format. Use 'Text' to extract the text from the HTML or 'HTML' for the raw HTML content.",
								"name": "format",
								"options": ["Text", "HTML"],
								"options_metadata": [],
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"toggle": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "str",
								"value": "Text"
							},
							"headers": {
								"_input_type": "TableInput",
								"advanced": true,
								"display_name": "Headers",
								"dynamic": false,
								"info": "The headers to send with the request",
								"input_types": ["DataFrame"],
								"is_list": true,
								"list_add_label": "Add More",
								"name": "headers",
								"placeholder": "",
								"required": false,
								"show": true,
								"table_icon": "Table",
								"table_schema": {
									"columns": [
										{
											"default": "None",
											"description": "Header name",
											"disable_edit": false,
											"display_name": "Header",
											"edit_mode": "popover",
											"filterable": true,
											"formatter": "text",
											"hidden": false,
											"name": "key",
											"sortable": true,
											"type": "str"
										},
										{
											"default": "None",
											"description": "Header value",
											"disable_edit": false,
											"display_name": "Value",
											"edit_mode": "popover",
											"filterable": true,
											"formatter": "text",
											"hidden": false,
											"name": "value",
											"sortable": true,
											"type": "str"
										}
									]
								},
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"trigger_icon": "Table",
								"trigger_text": "Open table",
								"type": "table",
								"value": [
									{
										"key": "User-Agent",
										"value": "langflow"
									}
								]
							},
							"max_depth": {
								"_input_type": "IntInput",
								"advanced": false,
								"display_name": "Max Depth",
								"dynamic": false,
								"info": "Controls how many 'clicks' away from the initial page the crawler will go:\n- depth 1: only the initial page\n- depth 2: initial page + all pages linked directly from it\n- depth 3: initial page + direct links + links found on those direct link pages\nNote: This is about link traversal, not URL path depth.",
								"list": false,
								"list_add_label": "Add More",
								"name": "max_depth",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "int",
								"value": 1
							},
							"prevent_outside": {
								"_input_type": "BoolInput",
								"advanced": true,
								"display_name": "Prevent Outside",
								"dynamic": false,
								"info": "If enabled, only crawls URLs within the same domain as the root URL. This helps prevent the crawler from going to external websites.",
								"list": false,
								"list_add_label": "Add More",
								"name": "prevent_outside",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "bool",
								"value": true
							},
							"timeout": {
								"_input_type": "IntInput",
								"advanced": true,
								"display_name": "Timeout",
								"dynamic": false,
								"info": "Timeout for the request in seconds.",
								"list": false,
								"list_add_label": "Add More",
								"name": "timeout",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "int",
								"value": 30
							},
							"urls": {
								"_input_type": "MessageTextInput",
								"advanced": false,
								"display_name": "URLs",
								"dynamic": false,
								"info": "Enter one or more URLs to crawl recursively, by clicking the '+' button.",
								"input_types": ["Message"],
								"list": true,
								"list_add_label": "Add URL",
								"load_from_db": false,
								"name": "urls",
								"placeholder": "Enter a URL...",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": true,
								"trace_as_input": true,
								"trace_as_metadata": true,
								"type": "str",
								"value": ["http://localhost:5173/api"]
							},
							"use_async": {
								"_input_type": "BoolInput",
								"advanced": true,
								"display_name": "Use Async",
								"dynamic": false,
								"info": "If enabled, uses asynchronous loading which can be significantly faster but might use more system resources.",
								"list": false,
								"list_add_label": "Add More",
								"name": "use_async",
								"placeholder": "",
								"required": false,
								"show": true,
								"title_case": false,
								"tool_mode": false,
								"trace_as_metadata": true,
								"type": "bool",
								"value": true
							}
						},
						"tool_mode": false
					},
					"showNode": true,
					"type": "URLComponent"
				},
				"dragging": false,
				"id": "URLComponent-wTaji",
				"measured": {
					"height": 315,
					"width": 320
				},
				"position": {
					"x": -336.8564556434334,
					"y": 882.2940062155923
				},
				"selected": false,
				"type": "genericNode"
			}
		],
		"viewport": {
			"x": -204.85546330893203,
			"y": -401.5874479611201,
			"zoom": 0.5644591584585842
		}
	},
	"description": "A LangFlow flow defining a help desk chatbot that helps customers create well formed support tickets.",
	"endpoint_name": null,
	"id": "3c767a31-2a0f-4389-ab36-6001d45e7f99",
	"is_component": false,
	"last_tested_version": "1.4.2.dev8",
	"name": "Ahoy Matey",
	"tags": []
}
